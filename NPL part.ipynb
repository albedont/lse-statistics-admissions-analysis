{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29186a00",
   "metadata": {},
   "source": [
    "# Using Natural Language Processing to derive insights from BSc Data Science personal statements\n",
    "\n",
    "This contains 10 BSc Data Science personal statements representating 50% of the cohort starting in 2022\n",
    "\n",
    "Using NPL the following things will be identified from the personal statements:\n",
    "- Main topics (Using Topic Modelling)\n",
    "- Top 10 most frequent words - this may be more insightful than topic modelling as topic modelling is an unsupervised machine learning algorithm it is not always as effective as desired \n",
    "- Key people who have been mentioned - this could be used to provide a basis for potential students to do further research to explore their interests\n",
    "- Readability levels - This will be calculated using the Flesch-Kincaid Grade Level Forumla which calculates the grade level needed to understand a text using the average syllables per word and the average number of words per sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81d5b912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Personal statement:\\nTaking part in a coding competition and several UKMT challenges exposed me to new ways of approaching problems from what I was used to in school. The challenge of having to think differently about problems and eventually arrive at solutions after reaching dead-ends was something I relished doing, which convinced me that a quantitative degree was for me.\\nI have continued to challenge myself and explore the interplay between computer science and mathematics by solving Project Euler problems. For example, I used recursion to compute the convergents of continued fractions of the square root of 2. These problems nurtured my interest in pure mathematics, leading me to read Kevin Houston\\'s \"How to Think like a Mathematician\". This introduced me to more sophisticated inductive proof techniques than the ones I learned at A-level. To supplement this, I explored how Lean, a programming language, can be taught to inductively prove a simple addition from first principles. What e'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reading the text file\n",
    "with open('ST115_personal_statements.txt', 'r') as file:\n",
    "    all_ps = file.read()\n",
    "\n",
    "# Displaying the first 1000 characters of the text file\n",
    "all_ps[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993b02f3",
   "metadata": {},
   "source": [
    "Looking at the first 1000 characters shown above it can be seen that some intial data cleaning must be done in the sense of removing '\\n' and any backslashes in general.\n",
    "Also we want to manipulate this text data in order to store all the different personal statements as one text to do topic modelling with and to store them as individual statements to calculate readability and key people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a27966c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inital removal of '\\n' and backslashes\n",
    "all_ps = all_ps.replace(\"\\n\", \"\")\n",
    "all_ps = all_ps.replace(\"\\xa0\", \"\")\n",
    "all_ps = all_ps.replace(\"\\'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "047791ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = all_ps.replace(\"Personal statement:\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adf192d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Personal statement:Taking part in a coding competition and several UKMT challenges exposed me to new ways of approaching problems from what I was used to in school. The challenge of having to think differently about problems and eventually arrive at solutions after reaching dead-ends was something I relished doing, which convinced me that a quantitative degree was for me.I have continued to challenge myself and explore the interplay between computer science and mathematics by solving Project Euler problems. For example, I used recursion to compute the convergents of continued fractions of the square root of 2. These problems nurtured my interest in pure mathematics, leading me to read Kevin Houstons \"How to Think like a Mathematician\". This introduced me to more sophisticated inductive proof techniques than the ones I learned at A-level. To supplement this, I explored how Lean, a programming language, can be taught to inductively prove a simple addition from first principles. What enthralled me about this was how Lean wasnt originally programmed to perform additions but proved it by itself using Peanos axioms.Reading about how deep learning has been used in the Synthetic Brain Project has prompted me to delve into how the backpropagation algorithm works. I found that it has surprisingly low computational cost, requiring only one forward pass and one backward pass through the network to simultaneously compute the partial derivatives of the cost function with respect to the weights. To further my understanding of the underlying mathematics of backpropagation, I read Michael Nielsens \"Neural Networks and Deep Learning\". I explored how the fundamental backpropagation equations are recursively calculated to compute the gradient of the cost function using partial differentiation. To develop my interest in how calculus is used in optimisation, I have looked into how Lagrange multipliers can be introduced when constraints are placed on a function. As these methods typically only find local minima, I read a paper on how Bayesian optimisation is used to find global minima. What I liked about this method is how we could globally optimise very complex black-box functions by taking into accounts past evaluations when determining which points to evaluate next. The algorithm usually requires a small number of iterations, making it a good algorithm for hyperparameter tuning in neural networks.Last year, I completed an online machine learning course on SoloLearn, which gave me a broad view of different machine learning models, as well as different methods of preventing overfitting. Using what I had learned, I built several Logistic Regression and Random Forests models that analysed small datasets and evaluated their performance, using techniques such as k-fold cross-validation. Building on this, I decided to create a data analysis program for my A-level Computer Science coursework to analyse medical data using a neural network, which I am going to compare against the prototypes that I designed. During the planning phase of this project, I was introduced to data wrangling and data warehousing. By reading Wes McKinneys \"Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython\", I have gained a clearer idea of how I am going to initially clean and aggregate the data using the Pandas library. This is a crucial stage since the data will be collected from many diverse sources. The final results will be stored in a data warehouse for reporting. This was a research-led approach that required me to evaluate multiple methods, a thought process that I enjoyed going through.This degree will give me the opportunity to deepen my understanding of the fundamental mathematical and statistical underpinnings of the subject and equip me with the programming skills that I will need for a future career in machine learning research or data science. I will strive to make the most out of the opportunities during my time at your university.Personal statement:Mathematics and the social sciences are seen as opposites: you pick one or the other. Data science proves that is not true. When I picked up The Master Algorithm by Pedro Domingos I was astounded to see the role learning algorithms play in our day to day lives, how they weave a web around everyday interactions, feeding them to an almost infinite pool of information to tailor the smallest of things to our likings. From then, every single day I became more and more aware of their presence in my life, and in turn, an irresistible curiosity arised that urged me to get closer and closer to the field of data science and its different elements. Data science observes and analyses all types of interactions, and wanting to get closer to understanding them, I chose to study IB Higher Level Economics. The different components of the course led me to value the significance of the behaviours and choices of humans, from behavioural economic theory and the craft of incentives and packaging design, to governmental policies and their allocation of resources that lead to the formation of macroeconomic models that affect individuals just as much. I think data science is a key element that intrinsically links to economics, that observes, analyses and suggests decisions based on those choices.A key aspect of data science mentioned in Roger Peng’s The Art of Data Science is the effective communication of findings and suggestions. I’ve spent five years debating at a national and international level, and through this I have learned to apply and appreciate the skill of communication, leading to successes such as victory in various tournaments or receiving speaker awards. Without proper communication to teammates, the opposition team, and the judges, the nuance and purpose of carefully crafted arguments falls flat. The organisation of school wide events as a house captain has also made me reflect on the value of communication. I see that the use of the skill in debate, but also the feedback process used in planning events to improve the schools experiences, mirrors the cycle Peng discusses in his book that leads to improving the design of products and interfaces. But, without mathematics and computer science, data science can’t be complete. The mathematical foundation of data science is what allows it to be so powerful and influential in our society. Studying Higher Level Mathematics has taught me to look beyond the surface level solutions, to not just take the maths they teach me for granted, but apply and intertwine them in creative manners to find unique solutions. I knew that the programming languages that compose data science were a way to apply my maths knowledge in another manner, but at first they felt like an unapproachable element. I motivated myself to confront that barrier, and through online courses and programming challenges learned Python, and that impulse led me to use it in academic contexts, such as using linear regression to analyse my Higher Level Physics Internal Assessment raw data.Leading a project that aims to redistribute electronic devices for online learning to university students who cant afford them as part of the Global Social Leaders Competition showed me the importance of resource allocation in my country and the rest of the world. I think data science and its capability for social good is perfect to address this type of issues and even prevent them from occurring, further inspiring me to pursue this field. Its depth and potential lets me see myself in the future creating new macroeconomic models through artificial intelligence, or starting projects that change the way product development is approached, while engaging with various kinds of knowledge: something I value for my development as an individual. As a student who has always been caught between mathematics and the social sciences, data science and its particular curriculum fits not only my academic interests, but also my personal ambitions perfectly.Personal statement: Data Science appealed to me as it offers a new perspective on contemporary business. The ability to see a problem from more than an economic or mathematical point of view but a technological point of view too is vital as you can show definitive, quantitative proof. I want to learn more about Maths, such as modelling and analytical processes that can handle large data sets and how to use that data to inform decisions. I enjoy expanding my extra-curricular knowledge through online courses, watching educational videos, like TEDx Talks, or reading the Financial Times or TechCrunch articles. An article I read about ‘FinTech’, particularly insurance companies use of AI, fascinated me. It detailed how AI is used to process the copious amounts of data companies sort through more efficiently and at lower costs. Insurance firms can show market failures through asymmetric information and moral hazard, but improvements in technology allows firms to gather and verify more complete data. This led me to think about how AI can allow businesses to quickly adapt to possibilities like supply shocks or fraud simply by adjusting its parameters, showing how technology could be essential to business in the future. The innovations in automated machinery could cause technological unemployment but it could then create more jobs in other sectors as well as improve firms’ productive efficiency. I researched FinTech further by completing an informative online course by CFTE and through further reading which intrigued me about a career in FinTech, using technology with Maths and Economics to work more efficiently. I’m excited to learn more about how technology can be used to compile and process data as well as make businesses more lucrative while maintaining safety. In Maths and Further Maths, I enjoy the logical chains of reasoning like those shown in induction and hypothesis testing, as well as the clear modelling in statistics that has computing applications in, for example, machine learning. The mathematical side of Economics particularly interests me, especially econometrics. It’s intriguing to me how Maths can be used in other subjects to aid analysis such as price elasticity of demand or supply in Economics. I’ve also learnt to explain the causes and consequences of economic choices more logically and concisely. While I won’t be continuing with Chemistry, I’ll continue using the detailed mathematical analysis and modelling of data samples that I’ve learnt studying rates of reaction and achieving focused independent research of complex topics in my future studies.In January 2020, I was able to shadow various roles at Vyaire Medical’s accounting operation. I learnt about financial reporting, including the variety of roles involved, how they work together to communicate recent performance, areas to improve and the company’s future performance, if they changed specific aspects or continued as normal, using highly mathematical computer programs to simplify and speed up their financial processes. This made me think how 10 years ago companies would’ve not only struggled to communicate as fast and easily as they can now but also how modern programs have enabled businesses to manage, share and analyse data from anywhere. As shown by the growth of remote working which has allowed companies to reduce real estate costs and carbon emissions.At university, I’m looking forward to continuing with my hobbies, like weightlifting and helping to combat climate change, which I’ve been doing as my schools’ Environment Prefect. I’ll keep up the hard work and dedication I’ve put into my A-level subjects and seize any opportunity to build on the skills I’ve gained and expand my knowledge. I’ll strive to improve in every area I can through criticism and self-reflection, addressing my limitations and improving upon them to further my ability. I have a genuine interest in the topics I’ll be able to study and I’m excited to learn more.Personal statement:My interest in data analytics stems from an experiment by the University of California. In Kenya, comparing free bed nets with insecticide-treated ones, experts found that children who had paid $14 more for the latter were 30% less likely to get malaria. This conclusion not only encouraged Kenyans to eradicate malaria through buying treated bed nets, but also inspired me to pursue quantitative solutions to intractable social problems.Poverty reduction, as the UNs primary goal for sustainable development, has always been my great concern. My Economics knowledge shows that measuring poverty by Multidimensional Poverty Index (MPI) is vital for formulating poverty eradication strategies. The 10-dimensional index data of the MPI are obtained through the population and economic censuses. The news that China had mobilized 7 million people to implement the seventh census made me realize that getting MPI uses massive human resources and economic costs. To find other reliable indexes, I joined the Night Remote Sensing project by the Chinese Academy of Sciences. When the professor presented the night-light data difference between developed and underdeveloped areas, I gripped the strong correlation between night-light data and GDP. To quantify whether the data can measure poverty requires more statistical knowledge, so I learned multiple regression. After performing regression analysis on the ten indicators of MPI and the night-light data in 31 provinces from 2007 to 2009, I got the result that they were positively correlated. I had never thought that geographic data could interdisciplinarily measure poverty, boosting my great enthusiasm for collecting and analysing data.However, when leading economics society members to explore income inequality, I found that the sheer quantity of social data has added calculation difficulty. Susan Athey of Stanford University argued that data, computing power and algorithmic reform will enable social scientists to mine social data on a large scale. To explore how algorithms can be applied to tackling social problems, I took an online course on machine learning by Carnegie Mellon University. I was deeply impressed by a case of a British team taking 525860 images of London as a training set and using the city’s lifestyle features as visual signal labels for deep learning; the outcome has enabled them to predict the inequality of other cities without image data. This proved that machine learning algorithms have distinct merits in computing massive unstructured data to predict. Therefore, using machine learning to foresee and help government set policies in advance will be my future learning direction.With the epidemic normalization, data has become a panacea for government to contain the spread of the disease while the economy recovers. The consensus among public healthexperts that cities must identify, and isolate close contacts of infected people quickly is urging governments to adopt contact-tracing technology. Although my previous exploration has revealed that mining data can provide immense social utility, I reserve my doubts about the effectiveness of tracking technology. For example, how can algorithms spot different levels of intimacy among contacts in the same place? How do algorithms measure the interactive behaviour of epidemiological significance? Considering the impact of data mining results on epidemic control, we should rigorously evaluate the tracking efficiency of big data. Therefore, I am keen on learning advanced statistics and algorithms and further exploring big datas role in social issues.My passion for emerging social cultures also impels me to quantify the related social effects. Using text analysis, my EPQ project “How do Western and Eastern people’s attitude differ towards Li Ziqi’s videos” reveals that the West has a higher recognition for her videos. I will devote my persistence and research enthusiasm to my following study. I sincerely hope to get the chance.Personal statement: Growing up on the football pitch, I learnt that a player’s worth does not depend on the scorecard, but rather on their ability to create order within an inherently uncertain game. The desire to make sense of a messy world has motivated me for as long as I can remember, initiating diverse personal projects and extensive work experience. Perhaps most importantly, this desire has led me to pursue a career in data science.At school, I was fascinated by both the sciences and humanities, taking Advanced Program Mathematics and Advanced Program English as additional subjects. Studying mathematics makes me feel like I am doing my part to find order within the chaos. This is specifically true for trigonometry, where complex problems are reduced to meagre identities. This perspective aided me in national Olympiads, where I ranked within the top hundred students. I enjoy English because of literature’s ability to look past what is and allow me to imagine what could be. I too competed in literary competitions and achieved similar ends. When able to bring these fields together, I was in my element. This was demonstrated by the mathematical writing of my final dissertation arguing presidential sex scandals do not end political careers; an assignment that propelled me into the top 1% of History students nationally. I was awarded both the Genuine Intellectual Curiosity and Creative Writing awards and finished high school in the top 5% of five separate subjects. I was also elected to student representative council every year since the 8th grade. After high school, I completed a year at the University of Witwatersrand taking Actuarial Science, Mathematical Statistics, Mathematics, Economics and Computer Science. It has here I realised that I would not be content as a software developer but rather saw those skills as prerequisites to engage with the potential of other fields. Studying data science will allow me to operate on the intersection of my interests.I have dedicated a large amount of time to personal projects. My first relied on C# and Unity in an attempt to create a game. The quality of the finished product was irrelevant, rather, for the first time, I had created something truly my own and was adamant to do it again. From programming Instagram Bots in python to full stack web applications powered by React, Node.JS and Firebase, I became lost in a world which I felt powerful enough to alter. A notable project was the use of Selenium (a python-based web automatization technology) to frequently fetch the stock prices of given firms and save them to a database. From there, I could use simple statistical methods, such as fitting linear regression lines, to locate trends.This was my first experience playing with sizeable amounts of data and I decided that the power to create paled in comparison against the power to know. Another pivotal project was my founding of a small property company, using money raised to buy an apartment. This project taught me the importance of pursing my ideas and that with a bit of effort, even the most far-fetched of ventures may come to fruition.My first internship took place at the end of 2020 at the advertising start-up called MotionAds, who soon offered me a part-time software developer position. Here, I implemented a variety of technologies, from a third form normalising their databases to a server-controlled WhatsApp Bot. As part of a team, many people were relying on my software which taught me the meticulous nature required to build robust systems. My work experience was in research at my university, focusing on healthcare data. I admired the commitment of the team and enjoyed the academic environment. I realised I had found my childhood heroes, who with every paper published, were discovering just a little more and helping others do the same. I have found the touch humanity to provide the why, now I am just chasing the how.Personal statement:I have always been drawn to figures and data, but it is only in the past year that I have truly explored the vast field of data science. This field exists at the forefront between statistics, computer science and mathematics. I believe this interdisciplinarity to be a common denominator in all instances of significant progress in todays academic climate. A clear example of such progress is the AlphaFold programme in which artificial intelligence was used alongside existing biological expertise, a testament to the potential when disciplines coalesce. This multifaceted approach resonates with me and data science is exceptionally well-suited to this outlook. Experiencing hands-on computing and statistics through a range of third level modules, I now have a deeper appreciation for the practical value of data processing techniques. I have used R to analyse a dataset and produce a statistical report intended for someone with a non-academic background. I was also able to use python to perform image segmentation and object tracking to calculate a value for acceleration due to gravity. Each of these tasks has shown me the importance of perseverance when it becomes difficult to make meaningful progress. Within the data science society, I have been able to attend workshops relating to programming and using relevant packages and tools. I was exposed to a range of industry-level processes, teaching me the value of applicable and relevant skills. In order to satisfy my desire for continuous personal development, I have undertaken a number of personal programming projects. Over the summer I invested my time into learning the fundamentals of python and computer science through various online courses from MIT and other institutions. Once I was comfortable with my knowledge base, I began working on a basic python-based trading bot which would monitor market patterns, buying and selling stocks based on a naive trading strategy. This was one of the most difficult yet satisfying endeavours I have faced to date. Each step of the way proved to be a unique learning opportunity as I struggled to diagnose problem after problem with my limited experience. In realising that I lacked the mathematical understanding to properly grasp the inner-workings of the system I was designing, I self-taught the relevant linear algebra and calculus required. This allowed me to much more easily troubleshoot the errors I faced and produce a functional final product. I believe dedication and commitment were key factors in completing this project. Over the last two years, I have volunteered within my community. As well as working in a soup kitchen and a geriatric hospital, I have dedicated the majority of my time to an animal shelter in which I cared for injured, abused and abandoned dogs. During my time at the shelter, I led a team tasked with retraining an injured dog to walk with an amputated leg over the course of two months. This experience taught me the importance of having patience both in building relationships and when undertaking work. Being constantly surrounded by passionate and driven individuals was an aspect which I had never fully appreciated. Running for an athletics team has counterbalanced my stress and allowed me to develop skills that will complement my future work. I have shown consistent work ethic by training as a team twice weekly on top of individual workouts. I have experienced working with a team in high-pressure, demanding scenarios. I have learned to organise and prioritise effectively in order to balance academics with extracurricular activities. As I pursue a career in data science I hope to further my understanding of this discipline by continuing to tackle those oftentimes enigmatic problems and immersing myself in whatever situation I find myself in. Personal statement:Data is one of the most valuable tools that the economy can have. With 2.5 quintillion bytes of data generated every day, it has a great influence on risks taken and potential rewards and by using mathematics, data can be analysed to predict future behaviour. Enrolling in a Data Analytics Virtual Experience Program organised by the KPMG data analytics team allowed me to learn more about the discipline of data analytics. Assessing the quality of 3 datasets using Excel and drafting an email highlighting the data quality concerns allowed me to develop my communication and analytical skills. This experience taught me about how to filter out data issues using Excel and how to interpret this information to determine which customer should be selected to bring the most benefit for the company.However, a problem I encountered while doing this task was the length of time spent and the possible human error when analysing the datasets given. Rather than filtering every column on Excel, I think that using algorithms to automatically filter the data would not only be a lot faster but also a lot more accurate. This has diverted my attention to learn more about machine learning so I am participating in an online Python course which will allow me to write complex algorithms with speed and efficiency.As technological advancements are continuously being made, artificial intelligence (AI) is getting more complex and advanced. Machine learning is an aspect of data analytics I find particularly interesting because it helps machines to self improve as more data is processed. Through developing data evaluation skills, I researched an article by Business Insider about the uses of AI in corporate finance. I strongly believe that machines have the potential of minimising operating expenses and further improving productivity in the financial sector. In this article, U.S. Bank used deep learning to analyse consumer data and detect potential fraud risks. I think this is particularly useful because as well as being more efficient, AI allows corporate accountants and analysts to target sustainable long term growth. According to an intelligence insider report, using this algorithm has helped U.S. Bank to double its productivity, therefore, I think that AI is a significant tool for the future success of finance as machines become progressively more advantageous.Regularly participating in the Problem Solving Club at my school allowed me to develop my creativity in approaching problems. I have been able to use this in both my research and my virtual experience tasks to understand the context of the problem and identify a logical solution using quantitative methods.Taking part in an AI and Machine Learning for Business course online allowed me to discover the importance of data analytic thinking and predictive analytical models. I enjoyed learning about machine learning techniques and I was able to discuss my ideas with other students. Working as a team and learning from others helped me to find the most ideal solutions for the case studies given.Researching further, I read ‘Neural Networks and Deep Learning’ by Michael Nielsen. I enjoyed studying Calculus in mathematics at A-level so I was fascinated by how the chain rule can be used repeatedly in backpropagation to pick up more information from previous layers. In his book, Nielsen predicts that “deep learning is here to stay” and I believe that this is true. As AI is continuously evolving, such as China’s facial recognition technology, I think that deep neural networks will become more valuable in the future.I would like to research further about how artificial intelligence and the manipulation of mathematical models can be used to make accurate predictions. I hope that reading this degree can help me reach my goal of developing algorithms for intelligent agents to provide more detailed forecasts and automation in the future of data analytics.Personal statement:I couldn’t believe what I saw. After looking at Joy Buolamwini’s video called gender shades, there was only one thing in my mind: How can algorithmic bias exist? Together with Timmit Gebru, Buolamwini investigated data sets and discovered that skin-type and gender biases exist in facial recognition technologies employed by big companies. Their research received significant recognition and influenced decisions made by tech giants like IBM. Perhaps that was the point I was inevitably drawn to a career related to big data, which could only be achieved with a data science major that builds upon solid math and programming skills.A particular data science project that captured my attention was published by Richi tech. in 2019. They found that my home country’s mean center of population shifted toward cities in the North by 4 km within only nine years. This brought me to think about a question that was frequently asked while I was volunteering in rural areas, “Will you come back?” While tutoring elementary students, I noticed that what those children lacked was not simply educational resources, but consistent care from teachers and volunteers that kept on changing each year, reflecting how population outflow in rural areas is affecting students’ opportunity to obtain stable educational resources. I began to wonder, what if I could raise awareness on unidentified trends of social issues by publishing findings of my own data analysis? To achieve my dream, I realized that I have to take the first step.I embarked on my journey when I signed up for an introductory python course in a national university. Thrilled by seeing how a bunch of code was reduced to a single line of command by importing functions that other programmers had written, I began dreaming about publishing a library of my own to manipulate data in an even more efficient way. Similar to the logic behind mathematical manipulations, I was attracted to the logic behind coding, the logic that allowed me to reach the ultimate solution through trials and errors. To further my understanding in the two disciplines that I love, I signed up for computer science and statistics courses in my senior year and devoted my last summer in high school learning about ways to interpret data with programming, hoping that one day I can use data to resolve social inequality issues.Attending the virtual data science summer school held by CIVICA UK was another great leap towards accomplishing my ambition. Using R’s plot function and packages like dplyr and ggplot2, I manipulated datasets to visualize the life expectancy of countries across the world. Although the graph exhibited a generally increasing trend, the widening gap between the rate of growth for life expectancies between rich and poor countries reflected trends of economic inequality. That brought me a better understanding on how powerful programming really is: it is a tool to extract, store, and examine data to analyze the past and predict the future, a future that will be built upon vast amounts of data.With math and computer science, I can transform data into valuable information that can affect companies’ operations. With math and computer science, I can utilize data to build machine learning models that can tackle inequalities in the world. It is with the UK’s outstanding facilities that I will be able to achieve my ambitions with these two subjects, to explore beneath the tip of an iceberg that I have yet touched upon. On top of providing world-famous education, the UK laid down the foundation to modern data analysis when John Graunt first analyzed statistics about death rates during the bubonic plague. Not only will I benefit from the country’s rich history in data analysis, I will also provide an international perspective to the college and provide insights to real world issues by analyzing data with mathematical statistics and computer science, two disciplines that will help me to become the next Buolamwini.Personal statement: I believe that the intersection of mathematics and computer science is where humans transform from simply applying existing algorithms to real-world problems to a thorough understanding of how these algorithms work on the inside, giving a new and advanced perspective of them. I developed this view after studying Bayes’ theorem in more depth, having initially known only of its formula and pervasive use in modelling. I progressed from thinking of it as a mere procedural tool, to understanding its inner mechanism as a way to incorporate acquired knowledge in our regression models, improving their accuracy.I am looking forward to learning more about support vectormachines.I amamazedby howthey can be used in steganographic detection in digital imagesby classifying the points of data via the introduction of Z, A, and B axes. The Code Book by Simon Singh discusses how steganographic techniques were used as a first means for safe communication, being relatively unsafe compared to cryptographic ones. I was puzzled by the extent to which modern cryptographers still value steganography and decided to investigate this. I found that steganography is used as a sophisticated backup plan for cryptographers, in case that the Riemann hypothesis is proved to be true. In this way, steganography can be likened to the first barrier in a two-factor authentication process, rather than an independent approach.Mathematics has always held the key to learning for me, culminating recently in a silver award at theSeniorMathematicalChallenge. The problem set relied more on a logical side of maths and allowed me to utilise my existing knowledge to new and unfamiliar scenarios in the branch of maths known as combinatorics. My first introduction to the field of statistics was when I received a letter from the ONS requiring me to complete the census in 2021. As this is an uncommon practice in my home country, I decided to find out how the collected data are used and what for. I discovered that most of the discussions were around the narrowness of the questions, for example the question about one’s gender only provided two options, so transgender and non-binary people felt discriminated against and hesitant to answer. Consequently, the accumulated data might not have provided accurate statistics and insight into the population, misinforming classification machine-learning algorithms which process and categorise these data. This reveals how seemingly insignificant details of the data collection process culminate in bias towards ethnicity, gender, race,and age. To stay ethical, data scientists must be mindful of contemporary environments - allowing a broader range of inputs - to discern between different subgroups of people more accurately. I am intrigued by the immense utility of data science, but mindful also of the responsibility which comes with it. The field is unique in that data scientists may find themselves developing measures to combat exploitations of their own designs; it is this ongoing race for invention which makes the subject exciting to me. This curiosity is complemented by the industry’sbreadth and multi-dimensionality, which have inspired and encouraged me to gainthe professional knowledge required to tackle these problems.With any endeavour, success requires persistent and determined effort. Remembering my goal of becoming a professional taekwondo athlete, I committed my free time to making small steps towards it each day. By this method, I achieved the Master of Sports in taekwondo, became twice champion of Asia, Northern Palmyra Cup, Uzbekistan, and multiple national competitions. Now my aim has changed, but the skills I’ve gained will always support me. I intend to apply them into becoming a data scientist with an active intellect: one who understands the limits of her knowledge, takes the initiative to build upon them and who puts ethics on an equal footing with efficacy.Personal statement: Seeing how influential big data is in shaping modern practices, from autonomous driving to fraud detection, has driven my interest to explore how data is harnessed to benefit society. I find it fascinating that disciplines from statistics and mathematics to computer science can be correlated to gain insight from data and applied to solve real-world problems.The area of application I am particularly interested in is the realm of finance. My first encounter with big data was through stock trading, where I explored how machine learning algorithms are used to determine optimal trading strategies and predict trends. This research, specifically Andrew Ngs course on machine learning, broadened my knowledge of the computational methods used for data analysis. Having completed the course, I used the mathematical and programming knowledge to create a stock price forecasting model. I implemented a linear regression algorithm in Matlab, which then spurred me to learn more about the mathematical foundations of machine learning models.In order to gain an understanding of the probability theory underpinning statistical inference, I read The Drunkards Walk by Leonard Mlodinow. Bernoullis Golden Theorem highlights that a small number of examples from a distribution cannot accurately reflect the underlying probabilities, leading me to believe that the availability of large datasets, as well as our capability to train models effectively on them is important for the deployment of machine learning systems. The book also brought to my attention that previous beliefs and intuition hinder the human ability to assess uncertain events and led me to explore the applications of probabilistic modelling in a lecture by Professor Zoubin Ghahramani. What I found most compelling was the utilisation of Bayes theorem, as it allows for machine learning models to estimate the uncertainty of each prediction in the face of challenging data examples. I think that using this theorem is critical in increasing the reliability of models, accommodating greater implementation in industry where unfamiliar data examples are prevalent. Furthermore, Bayes theorem exacted the issue that machine learning models lean heavily on human guidance, piquing my interest in the increased automation of machine learning, an idea I would like to explore in my future studies.As data-driven methods are implemented on a larger scale, the social impact becomes more visible. An article in The Guardian indicated how the government uses vast amounts of data collected on individuals to nudge their behaviour. Whilst encroachment of privacy is the clear initial impact, I think there is also an underlying risk of loss of free will when single entities, with access to large volumes of data, can influence mass behaviour in their own interest. In order to use big data and machine learning models beneficially, I believe it is essential to be aware of the potential social issues. Alongside machine learning, my interest expands to the wider field of artificial intelligence. I am a founding member of the AI club at my school, where we experiment with AI models primarily using Google TensorFlow, allowing me to implement the mathematical and computational techniques I have learnt within an AI environment. Based on my experience creating a maths tuition business, I have had to find various approaches to explaining a single concept. This practice has helped me examine problems from multiple perspectives, building on my problem-solving skills.If the focus in science is learning from experience and evidence, then I believe the usage of big data is critical in providing the capacity for machine learning algorithms to produce more accurate insights. The potential in the increasing automation of machine learning and its applications inspires me to pursue further study in this rapidly evolving field. I aspire to utilise big data to find solutions to real-world problems and further develop modern practices.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824e2931",
   "metadata": {},
   "source": [
    "<b> STEP 1: Readability </b>\n",
    "- Creating a dataframe where each row accounts for a single personal statement. This will be used to calculate the individual readability scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e035694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This phrase has been used in the text file to indicate the start of a different personal statement\n",
    "individual_statements = all_ps.split('Personal statement:')\n",
    "\n",
    "ps_df = pd.DataFrame({'Personal Statements': individual_statements})\n",
    "\n",
    "#The first row is empty so it is removed\n",
    "ps_df = ps_df.drop(index=0).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee118d83",
   "metadata": {},
   "source": [
    "Creating the Flesch-Kincaid function to calculate the readability scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8343f8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def count_syllables(word):\n",
    "    '''\n",
    "    Helper function to count the number of sylables in a given word\n",
    "    Input: \n",
    "    word : string\n",
    "    \n",
    "    Output:\n",
    "    num_vowels : int\n",
    "    '''\n",
    "    num_vowels = len(re.findall(r'[aeiouy]+', word))\n",
    "\n",
    "    # Subtract the number of silent e's at the end of the word\n",
    "    if re.search(r'e$', word):\n",
    "        num_vowels -= 1\n",
    "\n",
    "    # Subtract one for each diphthong\n",
    "    num_vowels -= len(re.findall(r'[aeiouy]{2}', word))\n",
    "\n",
    "    # Add one if the word ends in \"-le\"\n",
    "    if re.search(r'le$', word):\n",
    "        num_vowels += 1\n",
    "\n",
    "    # Add one if the word is one syllable and ends in a consonant followed by \"y\"\n",
    "    if len(re.findall(r'^[^aeiouy]+[aeiouy]+[^aeiouy]+y$', word)):\n",
    "        num_vowels += 1\n",
    "\n",
    "    return max(1, num_vowels)\n",
    "\n",
    "def flesch_kincaid_grade(text):\n",
    "    '''\n",
    "    The function uses the Flesch-Kincaid formula to calculate the grade level required to be able to read a specificed text\n",
    "    \n",
    "    Inputs: \n",
    "    text : string\n",
    "    The text to calculate the readability level of \n",
    "    \n",
    "    Outputs: \n",
    "    grade_level : float\n",
    "    Grade age of readability \n",
    "    '''\n",
    "    text = text\n",
    "    sentences = text.split('.')\n",
    "    words = text.split()\n",
    "\n",
    "    # Calculate the average number of words per sentence\n",
    "    words_per_sentence = len(words) / len(sentences)\n",
    "\n",
    "    # Calculate the average number of syllables per word\n",
    "    syllables = 0\n",
    "    for word in words:\n",
    "        syllables += count_syllables(word)\n",
    "    syllables_per_word = syllables / len(words)\n",
    "\n",
    "    grade_level = 0.39 * words_per_sentence + 11.8 * syllables_per_word - 15.59\n",
    "    \n",
    "    grade_level = round(grade_level, 1)\n",
    "\n",
    "    return grade_level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1573e432",
   "metadata": {},
   "source": [
    "Applying the function to all the personal statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b59d3ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_df['Readability Age Grade'] = ps_df['Personal Statements'].apply(flesch_kincaid_grade)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "712b052a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Personal Statements</th>\n",
       "      <th>Readability Age Grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Taking part in a coding competition and severa...</td>\n",
       "      <td>12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mathematics and the social sciences are seen a...</td>\n",
       "      <td>14.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Science appealed to me as it offers a ne...</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My interest in data analytics stems from an ex...</td>\n",
       "      <td>13.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Growing up on the football pitch, I learnt th...</td>\n",
       "      <td>10.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I have always been drawn to figures and data, ...</td>\n",
       "      <td>12.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data is one of the most valuable tools that th...</td>\n",
       "      <td>11.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I couldn’t believe what I saw. After looking a...</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I believe that the intersection of mathematic...</td>\n",
       "      <td>13.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Seeing how influential big data is in shaping...</td>\n",
       "      <td>13.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Personal Statements  Readability Age Grade\n",
       "0  Taking part in a coding competition and severa...                   12.5\n",
       "1  Mathematics and the social sciences are seen a...                   14.5\n",
       "2   Data Science appealed to me as it offers a ne...                   14.0\n",
       "3  My interest in data analytics stems from an ex...                   13.4\n",
       "4   Growing up on the football pitch, I learnt th...                   10.6\n",
       "5  I have always been drawn to figures and data, ...                   12.1\n",
       "6  Data is one of the most valuable tools that th...                   11.8\n",
       "7  I couldn’t believe what I saw. After looking a...                   14.0\n",
       "8   I believe that the intersection of mathematic...                   13.7\n",
       "9   Seeing how influential big data is in shaping...                   13.7"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1506b0c",
   "metadata": {},
   "source": [
    "<b> STEP 2: NLP Preprocessing tasks</b> \n",
    "\n",
    "The following tasks will be carried out in this section: \n",
    "- Tokenisation\n",
    "- Stop word removal \n",
    "- POS tagging\n",
    "- Named Entity Recognition \n",
    "- TF-IDF\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee91ba8",
   "metadata": {},
   "source": [
    "Tokenisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8426a7df",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [49]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m full_text \u001b[38;5;241m=\u001b[39m full_text\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#Creating a dataframe to store all the steps of preprocessing\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m npl_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOriginal text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_text\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m tokens \u001b[38;5;241m=\u001b[39m word_tokenize(full_text)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:636\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    630\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    631\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    632\u001b[0m     )\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    635\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 636\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmrecords\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmrecords\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:502\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    494\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    495\u001b[0m         x\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[0;32m    497\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m x\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[0;32m    499\u001b[0m     ]\n\u001b[0;32m    500\u001b[0m     \u001b[38;5;66;03m# TODO: can we get rid of the dt64tz special case above?\u001b[39;00m\n\u001b[1;32m--> 502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:120\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 120\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:664\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    661\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPer-column arrays must each be 1-dimensional\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexes \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_lengths:\n\u001b[1;32m--> 664\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf using all scalar values, you must pass an index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m have_series:\n\u001b[0;32m    667\u001b[0m     index \u001b[38;5;241m=\u001b[39m union_indexes(indexes)\n",
      "\u001b[1;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "full_text = full_text.lower()\n",
    "\n",
    "#Creating a dataframe to store all the steps of preprocessing\n",
    "npl_df = pd.DataFrame({'Original text': full_text})\n",
    "\n",
    "tokens = word_tokenize(full_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ac84ba",
   "metadata": {},
   "source": [
    "Checking if there are any specific words that should be added to the list of stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c99a8aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'taking': 5,\n",
       " 'part': 5,\n",
       " 'in': 127,\n",
       " 'a': 118,\n",
       " 'coding': 2,\n",
       " 'competition': 2,\n",
       " 'and': 189,\n",
       " 'several': 2,\n",
       " 'ukmt': 1,\n",
       " 'challenges': 2,\n",
       " 'exposed': 2,\n",
       " 'me': 69,\n",
       " 'to': 268,\n",
       " 'new': 5,\n",
       " 'ways': 2,\n",
       " 'of': 185,\n",
       " 'approaching': 2,\n",
       " 'problems': 11,\n",
       " 'from': 27,\n",
       " 'what': 12,\n",
       " 'i': 191,\n",
       " 'was': 44,\n",
       " 'used': 21,\n",
       " 'school': 9,\n",
       " '.': 219,\n",
       " 'the': 285,\n",
       " 'challenge': 2,\n",
       " 'having': 4,\n",
       " 'think': 13,\n",
       " 'differently': 1,\n",
       " 'about': 24,\n",
       " 'eventually': 1,\n",
       " 'arrive': 1,\n",
       " 'at': 18,\n",
       " 'solutions': 6,\n",
       " 'after': 6,\n",
       " 'reaching': 1,\n",
       " 'dead-ends': 1,\n",
       " 'something': 3,\n",
       " 'relished': 1,\n",
       " 'doing': 4,\n",
       " ',': 246,\n",
       " 'which': 22,\n",
       " 'convinced': 1,\n",
       " 'that': 84,\n",
       " 'quantitative': 4,\n",
       " 'degree': 3,\n",
       " 'for': 48,\n",
       " 'me.i': 1,\n",
       " 'have': 30,\n",
       " 'continued': 3,\n",
       " 'myself': 5,\n",
       " 'explore': 7,\n",
       " 'interplay': 1,\n",
       " 'between': 8,\n",
       " 'computer': 13,\n",
       " 'science': 36,\n",
       " 'mathematics': 15,\n",
       " 'by': 41,\n",
       " 'solving': 2,\n",
       " 'project': 11,\n",
       " 'euler': 1,\n",
       " 'example': 5,\n",
       " 'recursion': 1,\n",
       " 'compute': 3,\n",
       " 'convergents': 1,\n",
       " 'fractions': 1,\n",
       " 'square': 1,\n",
       " 'root': 1,\n",
       " '2.': 1,\n",
       " 'these': 8,\n",
       " 'nurtured': 1,\n",
       " 'my': 90,\n",
       " 'interest': 8,\n",
       " 'pure': 1,\n",
       " 'leading': 4,\n",
       " 'read': 6,\n",
       " 'kevin': 1,\n",
       " 'houstons': 1,\n",
       " '``': 3,\n",
       " 'how': 42,\n",
       " 'like': 11,\n",
       " 'mathematician': 1,\n",
       " \"''\": 3,\n",
       " 'this': 52,\n",
       " 'introduced': 3,\n",
       " 'more': 35,\n",
       " 'sophisticated': 2,\n",
       " 'inductive': 1,\n",
       " 'proof': 2,\n",
       " 'techniques': 6,\n",
       " 'than': 4,\n",
       " 'ones': 3,\n",
       " 'learned': 6,\n",
       " 'a-level': 4,\n",
       " 'supplement': 1,\n",
       " 'explored': 4,\n",
       " 'lean': 3,\n",
       " 'programming': 11,\n",
       " 'language': 1,\n",
       " 'can': 32,\n",
       " 'be': 31,\n",
       " 'taught': 6,\n",
       " 'inductively': 1,\n",
       " 'prove': 1,\n",
       " 'simple': 2,\n",
       " 'addition': 1,\n",
       " 'first': 12,\n",
       " 'principles': 1,\n",
       " 'enthralled': 1,\n",
       " 'wasnt': 1,\n",
       " 'originally': 1,\n",
       " 'programmed': 1,\n",
       " 'perform': 2,\n",
       " 'additions': 1,\n",
       " 'but': 18,\n",
       " 'proved': 4,\n",
       " 'it': 24,\n",
       " 'itself': 1,\n",
       " 'using': 24,\n",
       " 'peanos': 1,\n",
       " 'axioms.reading': 1,\n",
       " 'deep': 7,\n",
       " 'learning': 41,\n",
       " 'has': 26,\n",
       " 'been': 7,\n",
       " 'synthetic': 1,\n",
       " 'brain': 1,\n",
       " 'prompted': 1,\n",
       " 'delve': 1,\n",
       " 'into': 9,\n",
       " 'backpropagation': 4,\n",
       " 'algorithm': 6,\n",
       " 'works': 1,\n",
       " 'found': 8,\n",
       " 'surprisingly': 1,\n",
       " 'low': 1,\n",
       " 'computational': 3,\n",
       " 'cost': 3,\n",
       " 'requiring': 2,\n",
       " 'only': 13,\n",
       " 'one': 9,\n",
       " 'forward': 3,\n",
       " 'pass': 2,\n",
       " 'backward': 1,\n",
       " 'through': 16,\n",
       " 'network': 2,\n",
       " 'simultaneously': 1,\n",
       " 'partial': 2,\n",
       " 'derivatives': 1,\n",
       " 'function': 4,\n",
       " 'with': 36,\n",
       " 'respect': 1,\n",
       " 'weights': 1,\n",
       " 'further': 14,\n",
       " 'understanding': 10,\n",
       " 'underlying': 3,\n",
       " 'michael': 2,\n",
       " 'nielsens': 1,\n",
       " 'neural': 5,\n",
       " 'networks': 3,\n",
       " 'fundamental': 2,\n",
       " 'equations': 1,\n",
       " 'are': 10,\n",
       " 'recursively': 1,\n",
       " 'calculated': 1,\n",
       " 'gradient': 1,\n",
       " 'differentiation': 1,\n",
       " 'develop': 5,\n",
       " 'calculus': 3,\n",
       " 'is': 49,\n",
       " 'optimisation': 2,\n",
       " 'looked': 1,\n",
       " 'lagrange': 1,\n",
       " 'multipliers': 1,\n",
       " 'when': 14,\n",
       " 'constraints': 1,\n",
       " 'placed': 1,\n",
       " 'on': 40,\n",
       " 'as': 58,\n",
       " 'methods': 6,\n",
       " 'typically': 1,\n",
       " 'find': 13,\n",
       " 'local': 1,\n",
       " 'minima': 2,\n",
       " 'paper': 2,\n",
       " 'bayesian': 1,\n",
       " 'global': 2,\n",
       " 'liked': 1,\n",
       " 'method': 2,\n",
       " 'we': 3,\n",
       " 'could': 9,\n",
       " 'globally': 1,\n",
       " 'optimise': 1,\n",
       " 'very': 1,\n",
       " 'complex': 5,\n",
       " 'black-box': 1,\n",
       " 'functions': 2,\n",
       " 'accounts': 1,\n",
       " 'past': 4,\n",
       " 'evaluations': 1,\n",
       " 'determining': 1,\n",
       " 'points': 2,\n",
       " 'evaluate': 3,\n",
       " 'next': 2,\n",
       " 'usually': 1,\n",
       " 'requires': 3,\n",
       " 'small': 5,\n",
       " 'number': 3,\n",
       " 'iterations': 1,\n",
       " 'making': 2,\n",
       " 'good': 2,\n",
       " 'hyperparameter': 1,\n",
       " 'tuning': 1,\n",
       " 'networks.last': 1,\n",
       " 'year': 6,\n",
       " 'completed': 3,\n",
       " 'an': 38,\n",
       " 'online': 9,\n",
       " 'machine': 23,\n",
       " 'course': 10,\n",
       " 'sololearn': 1,\n",
       " 'gave': 1,\n",
       " 'broad': 1,\n",
       " 'view': 4,\n",
       " 'different': 6,\n",
       " 'models': 14,\n",
       " 'well': 7,\n",
       " 'preventing': 1,\n",
       " 'overfitting': 1,\n",
       " 'had': 9,\n",
       " 'built': 2,\n",
       " 'logistic': 1,\n",
       " 'regression': 7,\n",
       " 'random': 1,\n",
       " 'forests': 1,\n",
       " 'analysed': 2,\n",
       " 'datasets': 5,\n",
       " 'evaluated': 1,\n",
       " 'their': 11,\n",
       " 'performance': 3,\n",
       " 'such': 8,\n",
       " 'k-fold': 1,\n",
       " 'cross-validation': 1,\n",
       " 'building': 3,\n",
       " 'decided': 4,\n",
       " 'create': 6,\n",
       " 'data': 106,\n",
       " 'analysis': 10,\n",
       " 'program': 4,\n",
       " 'coursework': 1,\n",
       " 'analyse': 5,\n",
       " 'medical': 2,\n",
       " 'am': 10,\n",
       " 'going': 3,\n",
       " 'compare': 1,\n",
       " 'against': 3,\n",
       " 'prototypes': 1,\n",
       " 'designed': 1,\n",
       " 'during': 4,\n",
       " 'planning': 2,\n",
       " 'phase': 1,\n",
       " 'wrangling': 2,\n",
       " 'warehousing': 1,\n",
       " 'reading': 4,\n",
       " 'wes': 1,\n",
       " 'mckinneys': 1,\n",
       " 'python': 7,\n",
       " ':': 6,\n",
       " 'pandas': 2,\n",
       " 'numpy': 1,\n",
       " 'ipython': 1,\n",
       " 'gained': 3,\n",
       " 'clearer': 1,\n",
       " 'idea': 2,\n",
       " 'initially': 2,\n",
       " 'clean': 1,\n",
       " 'aggregate': 1,\n",
       " 'library': 2,\n",
       " 'crucial': 1,\n",
       " 'stage': 1,\n",
       " 'since': 2,\n",
       " 'will': 20,\n",
       " 'collected': 3,\n",
       " 'many': 2,\n",
       " 'diverse': 2,\n",
       " 'sources': 1,\n",
       " 'final': 3,\n",
       " 'results': 2,\n",
       " 'stored': 1,\n",
       " 'warehouse': 1,\n",
       " 'reporting': 2,\n",
       " 'research-led': 1,\n",
       " 'approach': 2,\n",
       " 'required': 4,\n",
       " 'multiple': 4,\n",
       " 'thought': 2,\n",
       " 'process': 7,\n",
       " 'enjoyed': 4,\n",
       " 'through.this': 1,\n",
       " 'give': 1,\n",
       " 'opportunity': 4,\n",
       " 'deepen': 1,\n",
       " 'mathematical': 15,\n",
       " 'statistical': 5,\n",
       " 'underpinnings': 1,\n",
       " 'subject': 2,\n",
       " 'equip': 1,\n",
       " 'skills': 8,\n",
       " 'need': 1,\n",
       " 'future': 13,\n",
       " 'career': 5,\n",
       " 'research': 8,\n",
       " 'or': 10,\n",
       " 'strive': 2,\n",
       " 'make': 5,\n",
       " 'most': 9,\n",
       " 'out': 3,\n",
       " 'opportunities': 1,\n",
       " 'time': 8,\n",
       " 'your': 1,\n",
       " 'university': 9,\n",
       " 'social': 17,\n",
       " 'sciences': 4,\n",
       " 'seen': 1,\n",
       " 'opposites': 1,\n",
       " 'you': 3,\n",
       " 'pick': 2,\n",
       " 'other': 9,\n",
       " 'proves': 1,\n",
       " 'not': 13,\n",
       " 'true': 4,\n",
       " 'picked': 1,\n",
       " 'up': 7,\n",
       " 'master': 2,\n",
       " 'pedro': 1,\n",
       " 'domingos': 1,\n",
       " 'astounded': 1,\n",
       " 'see': 4,\n",
       " 'role': 2,\n",
       " 'algorithms': 14,\n",
       " 'play': 1,\n",
       " 'our': 5,\n",
       " 'day': 6,\n",
       " 'lives': 1,\n",
       " 'they': 8,\n",
       " 'weave': 1,\n",
       " 'web': 3,\n",
       " 'around': 2,\n",
       " 'everyday': 1,\n",
       " 'interactions': 2,\n",
       " 'feeding': 1,\n",
       " 'them': 12,\n",
       " 'almost': 1,\n",
       " 'infinite': 1,\n",
       " 'pool': 1,\n",
       " 'information': 5,\n",
       " 'tailor': 1,\n",
       " 'smallest': 1,\n",
       " 'things': 1,\n",
       " 'likings': 1,\n",
       " 'then': 4,\n",
       " 'every': 6,\n",
       " 'single': 4,\n",
       " 'became': 3,\n",
       " 'aware': 2,\n",
       " 'presence': 1,\n",
       " 'life': 3,\n",
       " 'turn': 1,\n",
       " 'irresistible': 1,\n",
       " 'curiosity': 3,\n",
       " 'arised': 1,\n",
       " 'urged': 1,\n",
       " 'get': 4,\n",
       " 'closer': 3,\n",
       " 'field': 8,\n",
       " 'its': 9,\n",
       " 'elements': 1,\n",
       " 'observes': 2,\n",
       " 'analyses': 2,\n",
       " 'all': 2,\n",
       " 'types': 1,\n",
       " 'wanting': 1,\n",
       " 'chose': 1,\n",
       " 'study': 4,\n",
       " 'ib': 1,\n",
       " 'higher': 4,\n",
       " 'level': 6,\n",
       " 'economics': 8,\n",
       " 'components': 1,\n",
       " 'led': 6,\n",
       " 'value': 7,\n",
       " 'significance': 2,\n",
       " 'behaviours': 1,\n",
       " 'choices': 2,\n",
       " 'humans': 2,\n",
       " 'behavioural': 1,\n",
       " 'economic': 6,\n",
       " 'theory': 2,\n",
       " 'craft': 1,\n",
       " 'incentives': 1,\n",
       " 'packaging': 1,\n",
       " 'design': 2,\n",
       " 'governmental': 1,\n",
       " 'policies': 2,\n",
       " 'allocation': 2,\n",
       " 'resources': 4,\n",
       " 'lead': 1,\n",
       " 'formation': 1,\n",
       " 'macroeconomic': 2,\n",
       " 'affect': 2,\n",
       " 'individuals': 3,\n",
       " 'just': 4,\n",
       " 'much': 2,\n",
       " 'key': 4,\n",
       " 'element': 3,\n",
       " 'intrinsically': 1,\n",
       " 'links': 1,\n",
       " 'suggests': 1,\n",
       " 'decisions': 3,\n",
       " 'based': 3,\n",
       " 'those': 5,\n",
       " 'choices.a': 1,\n",
       " 'aspect': 3,\n",
       " 'mentioned': 1,\n",
       " 'roger': 1,\n",
       " 'peng': 2,\n",
       " '’': 42,\n",
       " 's': 16,\n",
       " 'art': 1,\n",
       " 'effective': 1,\n",
       " 'communication': 6,\n",
       " 'findings': 2,\n",
       " 'suggestions': 1,\n",
       " 've': 8,\n",
       " 'spent': 2,\n",
       " 'five': 2,\n",
       " 'years': 4,\n",
       " 'debating': 1,\n",
       " 'national': 4,\n",
       " 'international': 2,\n",
       " 'apply': 4,\n",
       " 'appreciate': 1,\n",
       " 'skill': 2,\n",
       " 'successes': 1,\n",
       " 'victory': 1,\n",
       " 'various': 5,\n",
       " 'tournaments': 1,\n",
       " 'receiving': 1,\n",
       " 'speaker': 1,\n",
       " 'awards': 2,\n",
       " 'without': 3,\n",
       " 'proper': 1,\n",
       " 'teammates': 1,\n",
       " 'opposition': 1,\n",
       " 'team': 10,\n",
       " 'judges': 1,\n",
       " 'nuance': 1,\n",
       " 'purpose': 1,\n",
       " 'carefully': 1,\n",
       " 'crafted': 1,\n",
       " 'arguments': 1,\n",
       " 'falls': 1,\n",
       " 'flat': 1,\n",
       " 'organisation': 1,\n",
       " 'wide': 1,\n",
       " 'events': 3,\n",
       " 'house': 1,\n",
       " 'captain': 1,\n",
       " 'also': 14,\n",
       " 'made': 5,\n",
       " 'reflect': 2,\n",
       " 'use': 11,\n",
       " 'debate': 1,\n",
       " 'feedback': 1,\n",
       " 'improve': 5,\n",
       " 'schools': 2,\n",
       " 'experiences': 1,\n",
       " 'mirrors': 1,\n",
       " 'cycle': 1,\n",
       " 'discusses': 2,\n",
       " 'his': 2,\n",
       " 'book': 4,\n",
       " 'leads': 1,\n",
       " 'improving': 4,\n",
       " 'products': 1,\n",
       " 'interfaces': 1,\n",
       " 't': 3,\n",
       " 'complete': 3,\n",
       " 'foundation': 2,\n",
       " 'allows': 4,\n",
       " 'so': 5,\n",
       " 'powerful': 3,\n",
       " 'influential': 2,\n",
       " 'society': 4,\n",
       " 'studying': 6,\n",
       " 'look': 2,\n",
       " 'beyond': 1,\n",
       " 'surface': 1,\n",
       " 'take': 2,\n",
       " 'maths': 10,\n",
       " 'teach': 1,\n",
       " 'granted': 1,\n",
       " 'intertwine': 1,\n",
       " 'creative': 2,\n",
       " 'manners': 1,\n",
       " 'unique': 3,\n",
       " 'knew': 1,\n",
       " 'languages': 1,\n",
       " 'compose': 1,\n",
       " 'were': 8,\n",
       " 'way': 6,\n",
       " 'knowledge': 13,\n",
       " 'another': 3,\n",
       " 'manner': 1,\n",
       " 'felt': 3,\n",
       " 'unapproachable': 1,\n",
       " 'motivated': 2,\n",
       " 'confront': 1,\n",
       " 'barrier': 2,\n",
       " 'courses': 4,\n",
       " 'impulse': 1,\n",
       " 'academic': 4,\n",
       " 'contexts': 1,\n",
       " 'linear': 4,\n",
       " 'physics': 1,\n",
       " 'internal': 1,\n",
       " 'assessment': 1,\n",
       " 'raw': 1,\n",
       " 'data.leading': 1,\n",
       " 'aims': 1,\n",
       " 'redistribute': 1,\n",
       " 'electronic': 1,\n",
       " 'devices': 1,\n",
       " 'students': 6,\n",
       " 'who': 7,\n",
       " 'cant': 1,\n",
       " 'afford': 1,\n",
       " 'leaders': 1,\n",
       " 'showed': 1,\n",
       " 'importance': 5,\n",
       " 'resource': 1,\n",
       " 'country': 4,\n",
       " 'rest': 1,\n",
       " 'world': 6,\n",
       " 'capability': 2,\n",
       " 'perfect': 1,\n",
       " 'address': 1,\n",
       " 'type': 1,\n",
       " 'issues': 5,\n",
       " 'even': 3,\n",
       " 'prevent': 1,\n",
       " 'occurring': 1,\n",
       " 'inspiring': 1,\n",
       " 'pursue': 5,\n",
       " 'depth': 2,\n",
       " 'potential': 8,\n",
       " 'lets': 1,\n",
       " 'creating': 2,\n",
       " 'artificial': 5,\n",
       " 'intelligence': 6,\n",
       " 'starting': 1,\n",
       " 'projects': 4,\n",
       " 'change': 2,\n",
       " 'product': 3,\n",
       " 'development': 4,\n",
       " 'approached': 1,\n",
       " 'while': 7,\n",
       " 'engaging': 1,\n",
       " 'kinds': 1,\n",
       " 'individual': 2,\n",
       " 'student': 2,\n",
       " 'always': 5,\n",
       " 'caught': 1,\n",
       " 'particular': 2,\n",
       " 'curriculum': 1,\n",
       " 'fits': 1,\n",
       " 'interests': 2,\n",
       " 'personal': 5,\n",
       " 'ambitions': 2,\n",
       " 'perfectly': 1,\n",
       " 'appealed': 1,\n",
       " 'offers': 1,\n",
       " 'perspective': 4,\n",
       " 'contemporary': 2,\n",
       " 'business': 5,\n",
       " 'ability': 5,\n",
       " 'problem': 7,\n",
       " 'point': 3,\n",
       " 'technological': 3,\n",
       " 'too': 2,\n",
       " 'vital': 2,\n",
       " 'show': 2,\n",
       " 'definitive': 1,\n",
       " 'want': 1,\n",
       " 'learn': 6,\n",
       " 'modelling': 5,\n",
       " 'analytical': 3,\n",
       " 'processes': 3,\n",
       " 'handle': 1,\n",
       " 'large': 5,\n",
       " 'sets': 2,\n",
       " 'inform': 1,\n",
       " 'enjoy': 3,\n",
       " 'expanding': 1,\n",
       " 'extra-curricular': 1,\n",
       " 'watching': 1,\n",
       " 'educational': 3,\n",
       " 'videos': 3,\n",
       " 'tedx': 1,\n",
       " 'talks': 1,\n",
       " 'financial': 4,\n",
       " 'times': 1,\n",
       " 'techcrunch': 1,\n",
       " 'articles': 1,\n",
       " 'article': 4,\n",
       " '‘': 2,\n",
       " 'fintech': 3,\n",
       " 'particularly': 5,\n",
       " 'insurance': 2,\n",
       " 'companies': 6,\n",
       " 'ai': 12,\n",
       " 'fascinated': 3,\n",
       " 'detailed': 3,\n",
       " 'copious': 1,\n",
       " 'amounts': 4,\n",
       " 'sort': 1,\n",
       " 'efficiently': 2,\n",
       " 'lower': 1,\n",
       " 'costs': 3,\n",
       " 'firms': 4,\n",
       " 'market': 2,\n",
       " 'failures': 1,\n",
       " 'asymmetric': 1,\n",
       " 'moral': 1,\n",
       " 'hazard': 1,\n",
       " 'improvements': 1,\n",
       " 'technology': 8,\n",
       " 'gather': 1,\n",
       " 'verify': 1,\n",
       " 'allow': 4,\n",
       " 'businesses': 3,\n",
       " 'quickly': 2,\n",
       " 'adapt': 1,\n",
       " 'possibilities': 1,\n",
       " 'supply': 2,\n",
       " 'shocks': 1,\n",
       " 'fraud': 3,\n",
       " 'simply': 3,\n",
       " 'adjusting': 1,\n",
       " 'parameters': 1,\n",
       " 'showing': 1,\n",
       " 'essential': 2,\n",
       " 'innovations': 1,\n",
       " 'automated': 1,\n",
       " 'machinery': 1,\n",
       " 'cause': 1,\n",
       " 'unemployment': 1,\n",
       " 'jobs': 1,\n",
       " 'sectors': 1,\n",
       " 'productive': 1,\n",
       " 'efficiency': 2,\n",
       " 'researched': 2,\n",
       " 'completing': 2,\n",
       " 'informative': 1,\n",
       " 'cfte': 1,\n",
       " 'intrigued': 2,\n",
       " 'work': 9,\n",
       " 'm': 3,\n",
       " 'excited': 2,\n",
       " 'compile': 1,\n",
       " 'lucrative': 1,\n",
       " 'maintaining': 1,\n",
       " 'safety': 1,\n",
       " 'logical': 3,\n",
       " 'chains': 1,\n",
       " 'reasoning': 1,\n",
       " 'shown': 4,\n",
       " 'induction': 1,\n",
       " 'hypothesis': 2,\n",
       " 'testing': 1,\n",
       " 'clear': 3,\n",
       " 'statistics': 11,\n",
       " 'computing': 4,\n",
       " 'applications': 4,\n",
       " 'side': 2,\n",
       " 'especially': 1,\n",
       " 'econometrics': 1,\n",
       " 'intriguing': 1,\n",
       " 'subjects': 5,\n",
       " 'aid': 1,\n",
       " 'price': 2,\n",
       " 'elasticity': 1,\n",
       " 'demand': 1,\n",
       " 'learnt': 5,\n",
       " 'explain': 1,\n",
       " 'causes': 1,\n",
       " 'consequences': 1,\n",
       " 'logically': 1,\n",
       " 'concisely': 1,\n",
       " 'won': 1,\n",
       " 'continuing': 3,\n",
       " 'chemistry': 1,\n",
       " 'll': 4,\n",
       " 'continue': 1,\n",
       " 'samples': 1,\n",
       " 'rates': 2,\n",
       " 'reaction': 1,\n",
       " 'achieving': 1,\n",
       " 'focused': 1,\n",
       " 'independent': 2,\n",
       " 'topics': 2,\n",
       " 'studies.in': 1,\n",
       " 'january': 1,\n",
       " '2020': 2,\n",
       " 'able': 8,\n",
       " 'shadow': 1,\n",
       " 'roles': 2,\n",
       " 'vyaire': 1,\n",
       " 'accounting': 1,\n",
       " 'operation': 1,\n",
       " 'including': 1,\n",
       " 'variety': 2,\n",
       " 'involved': 1,\n",
       " 'together': 3,\n",
       " 'communicate': 2,\n",
       " 'recent': 1,\n",
       " 'areas': 4,\n",
       " 'company': 2,\n",
       " 'if': 2,\n",
       " 'changed': 2,\n",
       " 'specific': 1,\n",
       " 'aspects': 1,\n",
       " 'normal': 1,\n",
       " 'highly': 1,\n",
       " 'programs': 2,\n",
       " 'simplify': 1,\n",
       " 'speed': 2,\n",
       " '10': 1,\n",
       " 'ago': 1,\n",
       " 'would': 6,\n",
       " 'struggled': 2,\n",
       " 'fast': 1,\n",
       " 'easily': 2,\n",
       " 'now': 4,\n",
       " 'modern': 5,\n",
       " 'enabled': 2,\n",
       " 'manage': 1,\n",
       " 'share': 1,\n",
       " 'anywhere': 1,\n",
       " 'growth': 3,\n",
       " 'remote': 2,\n",
       " 'working': 5,\n",
       " 'allowed': 9,\n",
       " 'reduce': 1,\n",
       " 'real': 2,\n",
       " 'estate': 1,\n",
       " 'carbon': 1,\n",
       " 'emissions.at': 1,\n",
       " 'looking': 3,\n",
       " 'hobbies': 1,\n",
       " 'weightlifting': 1,\n",
       " 'helping': 2,\n",
       " 'combat': 2,\n",
       " 'climate': 2,\n",
       " 'environment': 3,\n",
       " 'prefect': 1,\n",
       " 'keep': 1,\n",
       " 'hard': 1,\n",
       " 'dedication': 2,\n",
       " 'put': 1,\n",
       " 'seize': 1,\n",
       " 'any': 2,\n",
       " 'build': 4,\n",
       " 'expand': 1,\n",
       " 'area': 2,\n",
       " 'criticism': 1,\n",
       " 'self-reflection': 1,\n",
       " 'addressing': 1,\n",
       " 'limitations': 1,\n",
       " 'upon': 5,\n",
       " 'genuine': 2,\n",
       " 'analytics': 6,\n",
       " 'stems': 1,\n",
       " 'experiment': 2,\n",
       " 'california': 1,\n",
       " 'kenya': 1,\n",
       " 'comparing': 1,\n",
       " 'free': 3,\n",
       " 'bed': 2,\n",
       " 'nets': 2,\n",
       " 'insecticide-treated': 1,\n",
       " 'experts': 1,\n",
       " 'children': 2,\n",
       " 'paid': 1,\n",
       " '$': 1,\n",
       " '14': 1,\n",
       " 'latter': 1,\n",
       " '30': 1,\n",
       " '%': 3,\n",
       " 'less': 1,\n",
       " 'likely': 1,\n",
       " 'malaria': 2,\n",
       " 'conclusion': 1,\n",
       " 'encouraged': 2,\n",
       " 'kenyans': 1,\n",
       " 'eradicate': 1,\n",
       " 'buying': 2,\n",
       " 'treated': 1,\n",
       " 'inspired': 2,\n",
       " 'intractable': 1,\n",
       " 'problems.poverty': 1,\n",
       " 'reduction': 1,\n",
       " 'uns': 1,\n",
       " 'primary': 1,\n",
       " 'goal': 3,\n",
       " 'sustainable': 2,\n",
       " 'great': 4,\n",
       " 'concern': 1,\n",
       " 'shows': 1,\n",
       " 'measuring': 1,\n",
       " 'poverty': 5,\n",
       " 'multidimensional': 1,\n",
       " 'index': 2,\n",
       " '(': 3,\n",
       " 'mpi': 4,\n",
       " ')': 3,\n",
       " 'formulating': 1,\n",
       " 'eradication': 1,\n",
       " 'strategies': 2,\n",
       " '10-dimensional': 1,\n",
       " 'obtained': 1,\n",
       " 'population': 4,\n",
       " 'censuses': 1,\n",
       " 'news': 1,\n",
       " 'china': 2,\n",
       " 'mobilized': 1,\n",
       " '7': 1,\n",
       " 'million': 1,\n",
       " 'people': 6,\n",
       " 'implement': 2,\n",
       " 'seventh': 1,\n",
       " 'census': 2,\n",
       " 'realize': 1,\n",
       " 'getting': 2,\n",
       " 'uses': 3,\n",
       " 'massive': 2,\n",
       " 'human': 4,\n",
       " 'reliable': 1,\n",
       " 'indexes': 1,\n",
       " 'joined': 1,\n",
       " 'night': 1,\n",
       " 'sensing': 1,\n",
       " 'chinese': 1,\n",
       " 'academy': 1,\n",
       " 'professor': 2,\n",
       " 'presented': 1,\n",
       " 'night-light': 3,\n",
       " 'difference': 1,\n",
       " 'developed': 2,\n",
       " 'underdeveloped': 1,\n",
       " 'gripped': 1,\n",
       " 'strong': 1,\n",
       " 'correlation': 1,\n",
       " 'gdp': 1,\n",
       " 'quantify': 2,\n",
       " 'whether': 1,\n",
       " 'measure': 3,\n",
       " 'performing': 1,\n",
       " 'ten': 1,\n",
       " 'indicators': 1,\n",
       " '31': 1,\n",
       " 'provinces': 1,\n",
       " '2007': 1,\n",
       " '2009': 1,\n",
       " 'got': 1,\n",
       " 'result': 1,\n",
       " 'positively': 1,\n",
       " 'correlated': 2,\n",
       " 'never': 2,\n",
       " 'geographic': 1,\n",
       " 'interdisciplinarily': 1,\n",
       " 'boosting': 1,\n",
       " 'enthusiasm': 2,\n",
       " 'collecting': 1,\n",
       " 'analysing': 2,\n",
       " 'data.however': 1,\n",
       " 'members': 1,\n",
       " 'income': 1,\n",
       " 'inequality': 4,\n",
       " 'sheer': 1,\n",
       " 'quantity': 1,\n",
       " 'added': 1,\n",
       " 'calculation': 1,\n",
       " 'difficulty': 1,\n",
       " 'susan': 1,\n",
       " 'athey': 1,\n",
       " 'stanford': 1,\n",
       " 'argued': 1,\n",
       " 'power': 3,\n",
       " 'algorithmic': 2,\n",
       " 'reform': 1,\n",
       " 'enable': 1,\n",
       " 'scientists': 3,\n",
       " 'mine': 1,\n",
       " 'scale': 2,\n",
       " 'applied': 2,\n",
       " 'tackling': 1,\n",
       " 'took': 2,\n",
       " 'carnegie': 1,\n",
       " 'mellon': 1,\n",
       " 'deeply': 1,\n",
       " 'impressed': 1,\n",
       " 'case': 3,\n",
       " 'british': 1,\n",
       " '525860': 1,\n",
       " 'images': 1,\n",
       " 'london': 1,\n",
       " 'training': 2,\n",
       " 'set': 3,\n",
       " 'city': 1,\n",
       " 'lifestyle': 1,\n",
       " 'features': 1,\n",
       " 'visual': 1,\n",
       " 'signal': 1,\n",
       " 'labels': 1,\n",
       " ';': 3,\n",
       " 'outcome': 1,\n",
       " 'predict': 5,\n",
       " 'cities': 3,\n",
       " 'image': 2,\n",
       " 'distinct': 1,\n",
       " 'merits': 1,\n",
       " 'unstructured': 1,\n",
       " 'therefore': 3,\n",
       " 'foresee': 1,\n",
       " 'help': 3,\n",
       " 'government': 3,\n",
       " 'advance': 1,\n",
       " 'direction.with': 1,\n",
       " 'epidemic': 2,\n",
       " 'normalization': 1,\n",
       " 'become': 4,\n",
       " 'panacea': 1,\n",
       " 'contain': 1,\n",
       " 'spread': 1,\n",
       " 'disease': 1,\n",
       " 'economy': 2,\n",
       " 'recovers': 1,\n",
       " 'consensus': 1,\n",
       " 'among': 2,\n",
       " 'public': 1,\n",
       " 'healthexperts': 1,\n",
       " 'must': 2,\n",
       " 'identify': 2,\n",
       " 'isolate': 1,\n",
       " 'close': 1,\n",
       " 'contacts': 2,\n",
       " 'infected': 1,\n",
       " 'urging': 1,\n",
       " 'governments': 1,\n",
       " 'adopt': 1,\n",
       " 'contact-tracing': 1,\n",
       " 'although': 2,\n",
       " 'previous': 3,\n",
       " 'exploration': 1,\n",
       " 'revealed': 1,\n",
       " 'mining': 2,\n",
       " 'provide': 5,\n",
       " 'immense': 2,\n",
       " 'utility': 2,\n",
       " 'reserve': 1,\n",
       " 'doubts': 1,\n",
       " 'effectiveness': 1,\n",
       " 'tracking': 3,\n",
       " 'spot': 1,\n",
       " 'levels': 1,\n",
       " 'intimacy': 1,\n",
       " 'same': 2,\n",
       " 'place': 2,\n",
       " '?': 5,\n",
       " 'do': 5,\n",
       " 'interactive': 1,\n",
       " 'behaviour': 4,\n",
       " 'epidemiological': 1,\n",
       " 'considering': 1,\n",
       " 'impact': 3,\n",
       " 'control': 1,\n",
       " 'should': 2,\n",
       " 'rigorously': 1,\n",
       " 'big': 9,\n",
       " 'keen': 1,\n",
       " 'advanced': 5,\n",
       " 'exploring': 1,\n",
       " 'datas': 1,\n",
       " 'issues.my': 1,\n",
       " 'passion': 1,\n",
       " 'emerging': 1,\n",
       " 'cultures': 1,\n",
       " 'impels': 1,\n",
       " ...}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_token_dict = {}\n",
    "\n",
    "for token in tokens:\n",
    "    if token in full_token_dict: \n",
    "        full_token_dict[token] += 1\n",
    "    else: \n",
    "        full_token_dict[token] = 1\n",
    "\n",
    "full_token_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c659dad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
