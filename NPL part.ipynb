{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab1140f9",
   "metadata": {},
   "source": [
    "# Using Natural Language Processing to derive insights from BSc Data Science personal statements\n",
    "\n",
    "This contains 10 BSc Data Science personal statements representating 50% of the cohort starting in 2022\n",
    "\n",
    "Using NPL the following things will be identified from the personal statements:\n",
    "- Main topics (Using Topic Modelling)\n",
    "- Top 10 most frequent words - this may be more insightful than topic modelling as topic modelling is an unsupervised machine learning algorithm it is not always as effective as desired \n",
    "- Key people who have been mentioned - this could be used to provide a basis for potential students to do further research to explore their interests\n",
    "- Readability levels - This will be calculated using the Flesch-Kincaid Grade Level Forumla which calculates the grade level needed to understand a text using the average syllables per word and the average number of words per sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6294d2fb",
   "metadata": {},
   "source": [
    "# NEED TO FIND THE KEY PEOPLE AND THE TOP 10 MOST FREQUENT WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "985651dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Personal statement:\\nTaking part in a coding competition and several UKMT challenges exposed me to new ways of approaching problems from what I was used to in school. The challenge of having to think differently about problems and eventually arrive at solutions after reaching dead-ends was something I relished doing, which convinced me that a quantitative degree was for me.\\nI have continued to challenge myself and explore the interplay between computer science and mathematics by solving Project Euler problems. For example, I used recursion to compute the convergents of continued fractions of the square root of 2. These problems nurtured my interest in pure mathematics, leading me to read Kevin Houston\\'s \"How to Think like a Mathematician\". This introduced me to more sophisticated inductive proof techniques than the ones I learned at A-level. To supplement this, I explored how Lean, a programming language, can be taught to inductively prove a simple addition from first principles. What e'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reading the text file\n",
    "with open('ST115_personal_statements.txt', 'r') as file:\n",
    "    all_ps = file.read()\n",
    "\n",
    "# Displaying the first 1000 characters of the text file\n",
    "all_ps[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26c25df",
   "metadata": {},
   "source": [
    "Looking at the first 1000 characters shown above it can be seen that some intial data cleaning must be done in the sense of removing '\\n' and any backslashes in general.\n",
    "Also we want to manipulate this text data in order to store all the different personal statements as one text to do topic modelling with and to store them as individual statements to calculate readability and key people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c376c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inital removal of '\\n' and backslashes\n",
    "all_ps = all_ps.replace(\"\\n\", \"\")\n",
    "all_ps = all_ps.replace(\"\\xa0\", \"\")\n",
    "all_ps = all_ps.replace(\"\\'\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d6dd03",
   "metadata": {},
   "source": [
    "<b> STEP 1: Creating a dataframe fro the personal statements </b>\n",
    "- Creating a dataframe where each row accounts for a single personal statement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ae5b0205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This phrase has been used in the text file to indicate the start of a different personal statement\n",
    "individual_statements = all_ps.split('Personal statement:')\n",
    "\n",
    "ps_df = pd.DataFrame({'Personal Statements': individual_statements})\n",
    "\n",
    "#The first row is empty so it is removed\n",
    "ps_df = ps_df.drop(index=0).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f3efe9",
   "metadata": {},
   "source": [
    "<b> STEP 2: Calculating Readability </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4cfed7",
   "metadata": {},
   "source": [
    "Creating the Flesch-Kincaid function to calculate the readability scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c20e145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def count_syllables(word):\n",
    "    '''\n",
    "    Helper function to count the number of sylables in a given word\n",
    "    Input: \n",
    "    word : string\n",
    "    \n",
    "    Output:\n",
    "    num_vowels : int\n",
    "    '''\n",
    "    num_vowels = len(re.findall(r'[aeiouy]+', word))\n",
    "\n",
    "    # Subtract the number of silent e's at the end of the word\n",
    "    if re.search(r'e$', word):\n",
    "        num_vowels -= 1\n",
    "\n",
    "    # Subtract one for each diphthong\n",
    "    num_vowels -= len(re.findall(r'[aeiouy]{2}', word))\n",
    "\n",
    "    # Add one if the word ends in \"-le\"\n",
    "    if re.search(r'le$', word):\n",
    "        num_vowels += 1\n",
    "\n",
    "    # Add one if the word is one syllable and ends in a consonant followed by \"y\"\n",
    "    if len(re.findall(r'^[^aeiouy]+[aeiouy]+[^aeiouy]+y$', word)):\n",
    "        num_vowels += 1\n",
    "\n",
    "    return max(1, num_vowels)\n",
    "\n",
    "def flesch_kincaid_grade(text):\n",
    "    '''\n",
    "    The function uses the Flesch-Kincaid formula to calculate the grade level required to be able to read a specificed text\n",
    "    \n",
    "    Inputs: \n",
    "    text : string\n",
    "    The text to calculate the readability level of \n",
    "    \n",
    "    Outputs: \n",
    "    grade_level : float\n",
    "    Grade age of readability \n",
    "    '''\n",
    "    text = text\n",
    "    sentences = text.split('.')\n",
    "    words = text.split()\n",
    "\n",
    "    # Calculate the average number of words per sentence\n",
    "    words_per_sentence = len(words) / len(sentences)\n",
    "\n",
    "    # Calculate the average number of syllables per word\n",
    "    syllables = 0\n",
    "    for word in words:\n",
    "        syllables += count_syllables(word)\n",
    "    syllables_per_word = syllables / len(words)\n",
    "\n",
    "    grade_level = 0.39 * words_per_sentence + 11.8 * syllables_per_word - 15.59\n",
    "    \n",
    "    grade_level = round(grade_level, 1)\n",
    "\n",
    "    return grade_level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a77ebd0",
   "metadata": {},
   "source": [
    "Applying the function to all the personal statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9c8f09e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_df['Readability Age Grade'] = ps_df['Personal Statements'].apply(flesch_kincaid_grade)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "930ce18a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Personal Statements</th>\n",
       "      <th>Readability Age Grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Taking part in a coding competition and severa...</td>\n",
       "      <td>12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mathematics and the social sciences are seen a...</td>\n",
       "      <td>14.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Science appealed to me as it offers a ne...</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My interest in data analytics stems from an ex...</td>\n",
       "      <td>13.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Growing up on the football pitch, I learnt th...</td>\n",
       "      <td>10.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I have always been drawn to figures and data, ...</td>\n",
       "      <td>12.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data is one of the most valuable tools that th...</td>\n",
       "      <td>11.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I couldnâ€™t believe what I saw. After looking a...</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I believe that the intersection of mathematic...</td>\n",
       "      <td>13.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Seeing how influential big data is in shaping...</td>\n",
       "      <td>13.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Personal Statements  Readability Age Grade\n",
       "0  Taking part in a coding competition and severa...                   12.5\n",
       "1  Mathematics and the social sciences are seen a...                   14.5\n",
       "2   Data Science appealed to me as it offers a ne...                   14.0\n",
       "3  My interest in data analytics stems from an ex...                   13.4\n",
       "4   Growing up on the football pitch, I learnt th...                   10.6\n",
       "5  I have always been drawn to figures and data, ...                   12.1\n",
       "6  Data is one of the most valuable tools that th...                   11.8\n",
       "7  I couldnâ€™t believe what I saw. After looking a...                   14.0\n",
       "8   I believe that the intersection of mathematic...                   13.7\n",
       "9   Seeing how influential big data is in shaping...                   13.7"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79356ba9",
   "metadata": {},
   "source": [
    "<b> STEP 3: NLP Preprocessing tasks</b> \n",
    "\n",
    "The following tasks will be carried out in this section: \n",
    "- Tokenisation\n",
    "- Stop word removal\n",
    "- Lemmatisation\n",
    "- POS tagging\n",
    "- Named Entity Recognition \n",
    "- TF-IDF\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c2b689",
   "metadata": {},
   "source": [
    "<b>Tokenisation:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "a30dd843",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'remove_punctuation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [166]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[0;32m      4\u001b[0m full_text \u001b[38;5;241m=\u001b[39m full_text\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m----> 6\u001b[0m ps_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m ps_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPersonal Statements\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mlower())\u001b[38;5;241m.\u001b[39mapply(word_tokenize)\u001b[38;5;241m.\u001b[39mapply(\u001b[43mremove_punctuation\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'remove_punctuation' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "full_text = full_text.lower()\n",
    "\n",
    "ps_df['Tokens'] = ps_df['Personal Statements'].apply(lambda x: x.lower()).apply(word_tokenize).apply(remove_punctuation)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9289b1",
   "metadata": {},
   "source": [
    "<b> Removing stop words </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "bdc51f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('English')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "09fec448",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokens = [token for token in tokens if token not in sw]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79824e1c",
   "metadata": {},
   "source": [
    "Checking if there are any words that are still in tokens and not in the NLTK stopwords that should be taken out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "fa365870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 246),\n",
       " ('.', 219),\n",
       " ('data', 106),\n",
       " ('â€™', 42),\n",
       " ('learning', 41),\n",
       " ('science', 36),\n",
       " ('using', 24),\n",
       " ('machine', 23),\n",
       " ('used', 21),\n",
       " ('social', 17)]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_token_dict = {}\n",
    "\n",
    "for token in filtered_tokens:\n",
    "    if token in full_token_dict: \n",
    "        full_token_dict[token] += 1\n",
    "    else: \n",
    "        full_token_dict[token] = 1\n",
    "\n",
    "sorted_dict = sorted(full_token_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "sorted_dict[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0ba898",
   "metadata": {},
   "source": [
    "As can be seen the only tokens that were not in stop words that should be taken out are punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d481c414",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking out punctuation\n",
    "filtered_tokens = [token for token in filtered_tokens if token not in ['.', ',', \"'\", 'â€™', 'â€˜', '[', ']', ':', \"''\", '``']]\n",
    "\n",
    "nlp_df['Filtered Tokens'] = [filtered_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0ddf2d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['taking',\n",
       " 'part',\n",
       " 'coding',\n",
       " 'competition',\n",
       " 'several',\n",
       " 'ukmt',\n",
       " 'challenges',\n",
       " 'exposed',\n",
       " 'new',\n",
       " 'ways',\n",
       " 'approaching',\n",
       " 'problems',\n",
       " 'used',\n",
       " 'school',\n",
       " 'challenge',\n",
       " 'think',\n",
       " 'differently',\n",
       " 'problems',\n",
       " 'eventually',\n",
       " 'arrive',\n",
       " 'solutions',\n",
       " 'reaching',\n",
       " 'dead-ends',\n",
       " 'something',\n",
       " 'relished',\n",
       " 'convinced',\n",
       " 'quantitative',\n",
       " 'degree',\n",
       " 'me.i',\n",
       " 'continued',\n",
       " 'challenge',\n",
       " 'explore',\n",
       " 'interplay',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'mathematics',\n",
       " 'solving',\n",
       " 'project',\n",
       " 'euler',\n",
       " 'problems',\n",
       " 'example',\n",
       " 'used',\n",
       " 'recursion',\n",
       " 'compute',\n",
       " 'convergents',\n",
       " 'continued',\n",
       " 'fractions',\n",
       " 'square',\n",
       " 'root',\n",
       " '2.',\n",
       " 'problems',\n",
       " 'nurtured',\n",
       " 'interest',\n",
       " 'pure',\n",
       " 'mathematics',\n",
       " 'leading',\n",
       " 'read',\n",
       " 'kevin',\n",
       " 'houstons',\n",
       " 'think',\n",
       " 'like',\n",
       " 'mathematician',\n",
       " 'introduced',\n",
       " 'sophisticated',\n",
       " 'inductive',\n",
       " 'proof',\n",
       " 'techniques',\n",
       " 'ones',\n",
       " 'learned',\n",
       " 'a-level',\n",
       " 'supplement',\n",
       " 'explored',\n",
       " 'lean',\n",
       " 'programming',\n",
       " 'language',\n",
       " 'taught',\n",
       " 'inductively',\n",
       " 'prove',\n",
       " 'simple',\n",
       " 'addition',\n",
       " 'first',\n",
       " 'principles',\n",
       " 'enthralled',\n",
       " 'lean',\n",
       " 'wasnt',\n",
       " 'originally',\n",
       " 'programmed',\n",
       " 'perform',\n",
       " 'additions',\n",
       " 'proved',\n",
       " 'using',\n",
       " 'peanos',\n",
       " 'axioms.reading',\n",
       " 'deep',\n",
       " 'learning',\n",
       " 'used',\n",
       " 'synthetic',\n",
       " 'brain',\n",
       " 'project',\n",
       " 'prompted',\n",
       " 'delve',\n",
       " 'backpropagation',\n",
       " 'algorithm',\n",
       " 'works',\n",
       " 'found',\n",
       " 'surprisingly',\n",
       " 'low',\n",
       " 'computational',\n",
       " 'cost',\n",
       " 'requiring',\n",
       " 'one',\n",
       " 'forward',\n",
       " 'pass',\n",
       " 'one',\n",
       " 'backward',\n",
       " 'pass',\n",
       " 'network',\n",
       " 'simultaneously',\n",
       " 'compute',\n",
       " 'partial',\n",
       " 'derivatives',\n",
       " 'cost',\n",
       " 'function',\n",
       " 'respect',\n",
       " 'weights',\n",
       " 'understanding',\n",
       " 'underlying',\n",
       " 'mathematics',\n",
       " 'backpropagation',\n",
       " 'read',\n",
       " 'michael',\n",
       " 'nielsens',\n",
       " 'neural',\n",
       " 'networks',\n",
       " 'deep',\n",
       " 'learning',\n",
       " 'explored',\n",
       " 'fundamental',\n",
       " 'backpropagation',\n",
       " 'equations',\n",
       " 'recursively',\n",
       " 'calculated',\n",
       " 'compute',\n",
       " 'gradient',\n",
       " 'cost',\n",
       " 'function',\n",
       " 'using',\n",
       " 'partial',\n",
       " 'differentiation',\n",
       " 'develop',\n",
       " 'interest',\n",
       " 'calculus',\n",
       " 'used',\n",
       " 'optimisation',\n",
       " 'looked',\n",
       " 'lagrange',\n",
       " 'multipliers',\n",
       " 'introduced',\n",
       " 'constraints',\n",
       " 'placed',\n",
       " 'function',\n",
       " 'methods',\n",
       " 'typically',\n",
       " 'find',\n",
       " 'local',\n",
       " 'minima',\n",
       " 'read',\n",
       " 'paper',\n",
       " 'bayesian',\n",
       " 'optimisation',\n",
       " 'used',\n",
       " 'find',\n",
       " 'global',\n",
       " 'minima',\n",
       " 'liked',\n",
       " 'method',\n",
       " 'could',\n",
       " 'globally',\n",
       " 'optimise',\n",
       " 'complex',\n",
       " 'black-box',\n",
       " 'functions',\n",
       " 'taking',\n",
       " 'accounts',\n",
       " 'past',\n",
       " 'evaluations',\n",
       " 'determining',\n",
       " 'points',\n",
       " 'evaluate',\n",
       " 'next',\n",
       " 'algorithm',\n",
       " 'usually',\n",
       " 'requires',\n",
       " 'small',\n",
       " 'number',\n",
       " 'iterations',\n",
       " 'making',\n",
       " 'good',\n",
       " 'algorithm',\n",
       " 'hyperparameter',\n",
       " 'tuning',\n",
       " 'neural',\n",
       " 'networks.last',\n",
       " 'year',\n",
       " 'completed',\n",
       " 'online',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'course',\n",
       " 'sololearn',\n",
       " 'gave',\n",
       " 'broad',\n",
       " 'view',\n",
       " 'different',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'models',\n",
       " 'well',\n",
       " 'different',\n",
       " 'methods',\n",
       " 'preventing',\n",
       " 'overfitting',\n",
       " 'using',\n",
       " 'learned',\n",
       " 'built',\n",
       " 'several',\n",
       " 'logistic',\n",
       " 'regression',\n",
       " 'random',\n",
       " 'forests',\n",
       " 'models',\n",
       " 'analysed',\n",
       " 'small',\n",
       " 'datasets',\n",
       " 'evaluated',\n",
       " 'performance',\n",
       " 'using',\n",
       " 'techniques',\n",
       " 'k-fold',\n",
       " 'cross-validation',\n",
       " 'building',\n",
       " 'decided',\n",
       " 'create',\n",
       " 'data',\n",
       " 'analysis',\n",
       " 'program',\n",
       " 'a-level',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'coursework',\n",
       " 'analyse',\n",
       " 'medical',\n",
       " 'data',\n",
       " 'using',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'going',\n",
       " 'compare',\n",
       " 'prototypes',\n",
       " 'designed',\n",
       " 'planning',\n",
       " 'phase',\n",
       " 'project',\n",
       " 'introduced',\n",
       " 'data',\n",
       " 'wrangling',\n",
       " 'data',\n",
       " 'warehousing',\n",
       " 'reading',\n",
       " 'wes',\n",
       " 'mckinneys',\n",
       " 'python',\n",
       " 'data',\n",
       " 'analysis',\n",
       " 'data',\n",
       " 'wrangling',\n",
       " 'pandas',\n",
       " 'numpy',\n",
       " 'ipython',\n",
       " 'gained',\n",
       " 'clearer',\n",
       " 'idea',\n",
       " 'going',\n",
       " 'initially',\n",
       " 'clean',\n",
       " 'aggregate',\n",
       " 'data',\n",
       " 'using',\n",
       " 'pandas',\n",
       " 'library',\n",
       " 'crucial',\n",
       " 'stage',\n",
       " 'since',\n",
       " 'data',\n",
       " 'collected',\n",
       " 'many',\n",
       " 'diverse',\n",
       " 'sources',\n",
       " 'final',\n",
       " 'results',\n",
       " 'stored',\n",
       " 'data',\n",
       " 'warehouse',\n",
       " 'reporting',\n",
       " 'research-led',\n",
       " 'approach',\n",
       " 'required',\n",
       " 'evaluate',\n",
       " 'multiple',\n",
       " 'methods',\n",
       " 'thought',\n",
       " 'process',\n",
       " 'enjoyed',\n",
       " 'going',\n",
       " 'through.this',\n",
       " 'degree',\n",
       " 'give',\n",
       " 'opportunity',\n",
       " 'deepen',\n",
       " 'understanding',\n",
       " 'fundamental',\n",
       " 'mathematical',\n",
       " 'statistical',\n",
       " 'underpinnings',\n",
       " 'subject',\n",
       " 'equip',\n",
       " 'programming',\n",
       " 'skills',\n",
       " 'need',\n",
       " 'future',\n",
       " 'career',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'research',\n",
       " 'data',\n",
       " 'science',\n",
       " 'strive',\n",
       " 'make',\n",
       " 'opportunities',\n",
       " 'time',\n",
       " 'university',\n",
       " 'mathematics',\n",
       " 'social',\n",
       " 'sciences',\n",
       " 'seen',\n",
       " 'opposites',\n",
       " 'pick',\n",
       " 'one',\n",
       " 'data',\n",
       " 'science',\n",
       " 'proves',\n",
       " 'true',\n",
       " 'picked',\n",
       " 'master',\n",
       " 'algorithm',\n",
       " 'pedro',\n",
       " 'domingos',\n",
       " 'astounded',\n",
       " 'see',\n",
       " 'role',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " 'play',\n",
       " 'day',\n",
       " 'day',\n",
       " 'lives',\n",
       " 'weave',\n",
       " 'web',\n",
       " 'around',\n",
       " 'everyday',\n",
       " 'interactions',\n",
       " 'feeding',\n",
       " 'almost',\n",
       " 'infinite',\n",
       " 'pool',\n",
       " 'information',\n",
       " 'tailor',\n",
       " 'smallest',\n",
       " 'things',\n",
       " 'likings',\n",
       " 'every',\n",
       " 'single',\n",
       " 'day',\n",
       " 'became',\n",
       " 'aware',\n",
       " 'presence',\n",
       " 'life',\n",
       " 'turn',\n",
       " 'irresistible',\n",
       " 'curiosity',\n",
       " 'arised',\n",
       " 'urged',\n",
       " 'get',\n",
       " 'closer',\n",
       " 'closer',\n",
       " 'field',\n",
       " 'data',\n",
       " 'science',\n",
       " 'different',\n",
       " 'elements',\n",
       " 'data',\n",
       " 'science',\n",
       " 'observes',\n",
       " 'analyses',\n",
       " 'types',\n",
       " 'interactions',\n",
       " 'wanting',\n",
       " 'get',\n",
       " 'closer',\n",
       " 'understanding',\n",
       " 'chose',\n",
       " 'study',\n",
       " 'ib',\n",
       " 'higher',\n",
       " 'level',\n",
       " 'economics',\n",
       " 'different',\n",
       " 'components',\n",
       " 'course',\n",
       " 'led',\n",
       " 'value',\n",
       " 'significance',\n",
       " 'behaviours',\n",
       " 'choices',\n",
       " 'humans',\n",
       " 'behavioural',\n",
       " 'economic',\n",
       " 'theory',\n",
       " 'craft',\n",
       " 'incentives',\n",
       " 'packaging',\n",
       " 'design',\n",
       " 'governmental',\n",
       " 'policies',\n",
       " 'allocation',\n",
       " 'resources',\n",
       " 'lead',\n",
       " 'formation',\n",
       " 'macroeconomic',\n",
       " 'models',\n",
       " 'affect',\n",
       " 'individuals',\n",
       " 'much',\n",
       " 'think',\n",
       " 'data',\n",
       " 'science',\n",
       " 'key',\n",
       " 'element',\n",
       " 'intrinsically',\n",
       " 'links',\n",
       " 'economics',\n",
       " 'observes',\n",
       " 'analyses',\n",
       " 'suggests',\n",
       " 'decisions',\n",
       " 'based',\n",
       " 'choices.a',\n",
       " 'key',\n",
       " 'aspect',\n",
       " 'data',\n",
       " 'science',\n",
       " 'mentioned',\n",
       " 'roger',\n",
       " 'peng',\n",
       " 'art',\n",
       " 'data',\n",
       " 'science',\n",
       " 'effective',\n",
       " 'communication',\n",
       " 'findings',\n",
       " 'suggestions',\n",
       " 'spent',\n",
       " 'five',\n",
       " 'years',\n",
       " 'debating',\n",
       " 'national',\n",
       " 'international',\n",
       " 'level',\n",
       " 'learned',\n",
       " 'apply',\n",
       " 'appreciate',\n",
       " 'skill',\n",
       " 'communication',\n",
       " 'leading',\n",
       " 'successes',\n",
       " 'victory',\n",
       " 'various',\n",
       " 'tournaments',\n",
       " 'receiving',\n",
       " 'speaker',\n",
       " 'awards',\n",
       " 'without',\n",
       " 'proper',\n",
       " 'communication',\n",
       " 'teammates',\n",
       " 'opposition',\n",
       " 'team',\n",
       " 'judges',\n",
       " 'nuance',\n",
       " 'purpose',\n",
       " 'carefully',\n",
       " 'crafted',\n",
       " 'arguments',\n",
       " 'falls',\n",
       " 'flat',\n",
       " 'organisation',\n",
       " 'school',\n",
       " 'wide',\n",
       " 'events',\n",
       " 'house',\n",
       " 'captain',\n",
       " 'also',\n",
       " 'made',\n",
       " 'reflect',\n",
       " 'value',\n",
       " 'communication',\n",
       " 'see',\n",
       " 'use',\n",
       " 'skill',\n",
       " 'debate',\n",
       " 'also',\n",
       " 'feedback',\n",
       " 'process',\n",
       " 'used',\n",
       " 'planning',\n",
       " 'events',\n",
       " 'improve',\n",
       " 'schools',\n",
       " 'experiences',\n",
       " 'mirrors',\n",
       " 'cycle',\n",
       " 'peng',\n",
       " 'discusses',\n",
       " 'book',\n",
       " 'leads',\n",
       " 'improving',\n",
       " 'design',\n",
       " 'products',\n",
       " 'interfaces',\n",
       " 'without',\n",
       " 'mathematics',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'data',\n",
       " 'science',\n",
       " 'complete',\n",
       " 'mathematical',\n",
       " 'foundation',\n",
       " 'data',\n",
       " 'science',\n",
       " 'allows',\n",
       " 'powerful',\n",
       " 'influential',\n",
       " 'society',\n",
       " 'studying',\n",
       " 'higher',\n",
       " 'level',\n",
       " 'mathematics',\n",
       " 'taught',\n",
       " 'look',\n",
       " 'beyond',\n",
       " 'surface',\n",
       " 'level',\n",
       " 'solutions',\n",
       " 'take',\n",
       " 'maths',\n",
       " 'teach',\n",
       " 'granted',\n",
       " 'apply',\n",
       " 'intertwine',\n",
       " 'creative',\n",
       " 'manners',\n",
       " 'find',\n",
       " 'unique',\n",
       " 'solutions',\n",
       " 'knew',\n",
       " 'programming',\n",
       " 'languages',\n",
       " 'compose',\n",
       " 'data',\n",
       " 'science',\n",
       " 'way',\n",
       " 'apply',\n",
       " 'maths',\n",
       " 'knowledge',\n",
       " 'another',\n",
       " 'manner',\n",
       " 'first',\n",
       " 'felt',\n",
       " 'like',\n",
       " 'unapproachable',\n",
       " 'element',\n",
       " 'motivated',\n",
       " 'confront',\n",
       " 'barrier',\n",
       " 'online',\n",
       " 'courses',\n",
       " 'programming',\n",
       " 'challenges',\n",
       " 'learned',\n",
       " 'python',\n",
       " 'impulse',\n",
       " 'led',\n",
       " 'use',\n",
       " 'academic',\n",
       " 'contexts',\n",
       " 'using',\n",
       " 'linear',\n",
       " 'regression',\n",
       " 'analyse',\n",
       " 'higher',\n",
       " 'level',\n",
       " 'physics',\n",
       " 'internal',\n",
       " 'assessment',\n",
       " 'raw',\n",
       " 'data.leading',\n",
       " 'project',\n",
       " 'aims',\n",
       " 'redistribute',\n",
       " 'electronic',\n",
       " 'devices',\n",
       " 'online',\n",
       " 'learning',\n",
       " 'university',\n",
       " 'students',\n",
       " 'cant',\n",
       " 'afford',\n",
       " 'part',\n",
       " 'global',\n",
       " 'social',\n",
       " 'leaders',\n",
       " 'competition',\n",
       " 'showed',\n",
       " 'importance',\n",
       " 'resource',\n",
       " 'allocation',\n",
       " 'country',\n",
       " 'rest',\n",
       " 'world',\n",
       " 'think',\n",
       " 'data',\n",
       " 'science',\n",
       " 'capability',\n",
       " 'social',\n",
       " 'good',\n",
       " 'perfect',\n",
       " 'address',\n",
       " 'type',\n",
       " 'issues',\n",
       " 'even',\n",
       " 'prevent',\n",
       " 'occurring',\n",
       " 'inspiring',\n",
       " 'pursue',\n",
       " 'field',\n",
       " 'depth',\n",
       " 'potential',\n",
       " 'lets',\n",
       " 'see',\n",
       " 'future',\n",
       " 'creating',\n",
       " 'new',\n",
       " 'macroeconomic',\n",
       " 'models',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'starting',\n",
       " 'projects',\n",
       " 'change',\n",
       " 'way',\n",
       " 'product',\n",
       " 'development',\n",
       " 'approached',\n",
       " 'engaging',\n",
       " 'various',\n",
       " 'kinds',\n",
       " 'knowledge',\n",
       " 'something',\n",
       " 'value',\n",
       " 'development',\n",
       " 'individual',\n",
       " 'student',\n",
       " 'always',\n",
       " 'caught',\n",
       " 'mathematics',\n",
       " 'social',\n",
       " 'sciences',\n",
       " 'data',\n",
       " 'science',\n",
       " 'particular',\n",
       " 'curriculum',\n",
       " 'fits',\n",
       " 'academic',\n",
       " 'interests',\n",
       " 'also',\n",
       " 'personal',\n",
       " 'ambitions',\n",
       " 'perfectly',\n",
       " 'data',\n",
       " 'science',\n",
       " 'appealed',\n",
       " 'offers',\n",
       " 'new',\n",
       " 'perspective',\n",
       " 'contemporary',\n",
       " 'business',\n",
       " 'ability',\n",
       " 'see',\n",
       " 'problem',\n",
       " 'economic',\n",
       " 'mathematical',\n",
       " 'point',\n",
       " 'view',\n",
       " 'technological',\n",
       " 'point',\n",
       " 'view',\n",
       " 'vital',\n",
       " 'show',\n",
       " 'definitive',\n",
       " 'quantitative',\n",
       " 'proof',\n",
       " 'want',\n",
       " 'learn',\n",
       " 'maths',\n",
       " 'modelling',\n",
       " 'analytical',\n",
       " 'processes',\n",
       " 'handle',\n",
       " 'large',\n",
       " 'data',\n",
       " 'sets',\n",
       " 'use',\n",
       " 'data',\n",
       " 'inform',\n",
       " 'decisions',\n",
       " 'enjoy',\n",
       " 'expanding',\n",
       " 'extra-curricular',\n",
       " 'knowledge',\n",
       " 'online',\n",
       " 'courses',\n",
       " 'watching',\n",
       " 'educational',\n",
       " 'videos',\n",
       " 'like',\n",
       " 'tedx',\n",
       " 'talks',\n",
       " 'reading',\n",
       " 'financial',\n",
       " 'times',\n",
       " 'techcrunch',\n",
       " 'articles',\n",
       " 'article',\n",
       " 'read',\n",
       " 'fintech',\n",
       " 'particularly',\n",
       " 'insurance',\n",
       " 'companies',\n",
       " 'use',\n",
       " 'ai',\n",
       " 'fascinated',\n",
       " 'detailed',\n",
       " 'ai',\n",
       " 'used',\n",
       " 'process',\n",
       " 'copious',\n",
       " 'amounts',\n",
       " 'data',\n",
       " 'companies',\n",
       " 'sort',\n",
       " 'efficiently',\n",
       " 'lower',\n",
       " 'costs',\n",
       " 'insurance',\n",
       " 'firms',\n",
       " 'show',\n",
       " 'market',\n",
       " 'failures',\n",
       " 'asymmetric',\n",
       " 'information',\n",
       " 'moral',\n",
       " 'hazard',\n",
       " 'improvements',\n",
       " 'technology',\n",
       " 'allows',\n",
       " 'firms',\n",
       " 'gather',\n",
       " 'verify',\n",
       " 'complete',\n",
       " 'data',\n",
       " 'led',\n",
       " 'think',\n",
       " 'ai',\n",
       " 'allow',\n",
       " 'businesses',\n",
       " 'quickly',\n",
       " 'adapt',\n",
       " 'possibilities',\n",
       " 'like',\n",
       " 'supply',\n",
       " 'shocks',\n",
       " 'fraud',\n",
       " 'simply',\n",
       " 'adjusting',\n",
       " 'parameters',\n",
       " 'showing',\n",
       " 'technology',\n",
       " 'could',\n",
       " 'essential',\n",
       " 'business',\n",
       " 'future',\n",
       " 'innovations',\n",
       " 'automated',\n",
       " 'machinery',\n",
       " 'could',\n",
       " 'cause',\n",
       " 'technological',\n",
       " 'unemployment',\n",
       " 'could',\n",
       " 'create',\n",
       " 'jobs',\n",
       " 'sectors',\n",
       " 'well',\n",
       " 'improve',\n",
       " 'firms',\n",
       " 'productive',\n",
       " 'efficiency',\n",
       " 'researched',\n",
       " 'fintech',\n",
       " 'completing',\n",
       " 'informative',\n",
       " 'online',\n",
       " 'course',\n",
       " 'cfte',\n",
       " 'reading',\n",
       " 'intrigued',\n",
       " 'career',\n",
       " 'fintech',\n",
       " 'using',\n",
       " 'technology',\n",
       " 'maths',\n",
       " 'economics',\n",
       " 'work',\n",
       " 'efficiently',\n",
       " 'excited',\n",
       " 'learn',\n",
       " 'technology',\n",
       " 'used',\n",
       " 'compile',\n",
       " 'process',\n",
       " 'data',\n",
       " 'well',\n",
       " 'make',\n",
       " 'businesses',\n",
       " 'lucrative',\n",
       " 'maintaining',\n",
       " 'safety',\n",
       " 'maths',\n",
       " 'maths',\n",
       " 'enjoy',\n",
       " 'logical',\n",
       " 'chains',\n",
       " 'reasoning',\n",
       " 'like',\n",
       " 'shown',\n",
       " 'induction',\n",
       " 'hypothesis',\n",
       " 'testing',\n",
       " 'well',\n",
       " 'clear',\n",
       " 'modelling',\n",
       " 'statistics',\n",
       " 'computing',\n",
       " 'applications',\n",
       " 'example',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'mathematical',\n",
       " 'side',\n",
       " 'economics',\n",
       " 'particularly',\n",
       " 'interests',\n",
       " 'especially',\n",
       " 'econometrics',\n",
       " 'intriguing',\n",
       " 'maths',\n",
       " 'used',\n",
       " 'subjects',\n",
       " 'aid',\n",
       " 'analysis',\n",
       " 'price',\n",
       " 'elasticity',\n",
       " 'demand',\n",
       " 'supply',\n",
       " 'economics',\n",
       " 'also',\n",
       " 'learnt',\n",
       " 'explain',\n",
       " 'causes',\n",
       " 'consequences',\n",
       " 'economic',\n",
       " 'choices',\n",
       " 'logically',\n",
       " 'concisely',\n",
       " 'continuing',\n",
       " 'chemistry',\n",
       " 'continue',\n",
       " 'using',\n",
       " 'detailed',\n",
       " 'mathematical',\n",
       " 'analysis',\n",
       " 'modelling',\n",
       " 'data',\n",
       " 'samples',\n",
       " 'learnt',\n",
       " 'studying',\n",
       " 'rates',\n",
       " 'reaction',\n",
       " 'achieving',\n",
       " 'focused',\n",
       " 'independent',\n",
       " 'research',\n",
       " 'complex',\n",
       " 'topics',\n",
       " 'future',\n",
       " 'studies.in',\n",
       " 'january',\n",
       " '2020',\n",
       " 'able',\n",
       " 'shadow',\n",
       " 'various',\n",
       " 'roles',\n",
       " 'vyaire',\n",
       " 'medical',\n",
       " 'accounting',\n",
       " 'operation',\n",
       " 'learnt',\n",
       " 'financial',\n",
       " 'reporting',\n",
       " 'including',\n",
       " 'variety',\n",
       " 'roles',\n",
       " 'involved',\n",
       " 'work',\n",
       " 'together',\n",
       " 'communicate',\n",
       " 'recent',\n",
       " 'performance',\n",
       " 'areas',\n",
       " 'improve',\n",
       " 'company',\n",
       " 'future',\n",
       " 'performance',\n",
       " 'changed',\n",
       " 'specific',\n",
       " 'aspects',\n",
       " 'continued',\n",
       " 'normal',\n",
       " 'using',\n",
       " 'highly',\n",
       " 'mathematical',\n",
       " 'computer',\n",
       " 'programs',\n",
       " 'simplify',\n",
       " 'speed',\n",
       " 'financial',\n",
       " 'processes',\n",
       " 'made',\n",
       " 'think',\n",
       " '10',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'companies',\n",
       " 'would',\n",
       " 'struggled',\n",
       " 'communicate',\n",
       " 'fast',\n",
       " 'easily',\n",
       " 'also',\n",
       " 'modern',\n",
       " 'programs',\n",
       " 'enabled',\n",
       " 'businesses',\n",
       " 'manage',\n",
       " 'share',\n",
       " 'analyse',\n",
       " 'data',\n",
       " 'anywhere',\n",
       " 'shown',\n",
       " 'growth',\n",
       " 'remote',\n",
       " 'working',\n",
       " 'allowed',\n",
       " 'companies',\n",
       " 'reduce',\n",
       " 'real',\n",
       " 'estate',\n",
       " 'costs',\n",
       " 'carbon',\n",
       " ...]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7865de3e",
   "metadata": {},
   "source": [
    "<b> Lemmatisation and POS Tagging </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f4970f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "words_w_tags = pos_tag(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "48d9a5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    first_letter = tag[0]\n",
    "    if first_letter == 'J': return wordnet.ADJ\n",
    "    if first_letter == 'V': return wordnet.VERB\n",
    "    if first_letter == 'N': return wordnet.NOUN\n",
    "    if first_letter == 'R': return wordnet.ADV\n",
    "    return None \n",
    "\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "lemmatised_tokens = []\n",
    "for word, tag in words_w_tags:\n",
    "    pos = get_wordnet_pos(tag)\n",
    "    if pos is None:\n",
    "        lemma = lemmatiser.lemmatize(word)\n",
    "    else:\n",
    "        lemma = lemmatiser.lemmatize(word, pos)\n",
    "    lemmatised_tokens.append(lemma)\n",
    "\n",
    "nlp_df['Lemmatised Tokens'] = [lemmatised_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f57054f",
   "metadata": {},
   "source": [
    "<b> TF-IDF </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "5d6eba19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x1471 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1471 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# join lemmatized tokens into a single string representing the document\n",
    "document = ' '.join(lemmatised_tokens)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "tf_idf_matrix = vectorizer.fit_transform([document])\n",
    "\n",
    "tf_idf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb512c78",
   "metadata": {},
   "source": [
    "<b> STEP 4: Topic Modelling </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "e6abe9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LatentDirichletAllocation(random_state=42)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=10, random_state=42)\n",
    "\n",
    "lda_model.fit(tf_idf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "5823de3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "10 peng others organise optimisation based store operation discuss strive\n",
      "Topic #1:\n",
      "10 peng others organise optimisation based store operation discuss strive\n",
      "Topic #2:\n",
      "10 peng others organise optimisation based store operation discuss strive\n",
      "Topic #3:\n",
      "10 peng others organise optimisation based store operation discuss strive\n",
      "Topic #4:\n",
      "10 peng others organise optimisation based store operation discuss strive\n",
      "Topic #5:\n",
      "10 peng others organise optimisation based store operation discuss strive\n",
      "Topic #6:\n",
      "data use learn science machine find model problem allow social\n",
      "Topic #7:\n",
      "10 peng others organise optimisation based store operation discuss strive\n",
      "Topic #8:\n",
      "10 peng others organise optimisation based store operation discuss strive\n",
      "Topic #9:\n",
      "10 peng others organise optimisation based store operation discuss strive\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "n_top_words = 10\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()[:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9f87b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
