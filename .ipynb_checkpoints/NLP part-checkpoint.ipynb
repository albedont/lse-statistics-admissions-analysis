{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cab11e6e",
   "metadata": {},
   "source": [
    "# Using Natural Language Processing to derive insights from BSc Data Science personal statements\n",
    "\n",
    "This contains 10 BSc Data Science personal statements representating 50% of the cohort starting in 2022\n",
    "\n",
    "Using NPL the following things will be identified from the personal statements:\n",
    "- Main topics (Using Topic Modelling)\n",
    "- Top 10 most frequent words - this may be more insightful than topic modelling as topic modelling is an unsupervised machine learning algorithm it is not always as effective as desired \n",
    "- Key people who have been mentioned - this could be used to provide a basis for potential students to do further research to explore their interests\n",
    "- Readability levels - This will be calculated using the Flesch-Kincaid Grade Level Forumla which calculates the grade level needed to understand a text using the average syllables per word and the average number of words per sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "544f80ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Personal statement:\\nTaking part in a coding competition and several UKMT challenges exposed me to new ways of approaching problems from what I was used to in school. The challenge of having to think differently about problems and eventually arrive at solutions after reaching dead-ends was something I relished doing, which convinced me that a quantitative degree was for me.\\nI have continued to challenge myself and explore the interplay between computer science and mathematics by solving Project Euler problems. For example, I used recursion to compute the convergents of continued fractions of the square root of 2. These problems nurtured my interest in pure mathematics, leading me to read Kevin Houston\\'s \"How to Think like a Mathematician\". This introduced me to more sophisticated inductive proof techniques than the ones I learned at A-level. To supplement this, I explored how Lean, a programming language, can be taught to inductively prove a simple addition from first principles. What e'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reading the text file\n",
    "with open('ST115_personal_statements.txt', 'r') as file:\n",
    "    all_ps = file.read()\n",
    "\n",
    "# Displaying the first 1000 characters of the text file\n",
    "all_ps[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1502ad52",
   "metadata": {},
   "source": [
    "Looking at the first 1000 characters shown above it can be seen that some intial data cleaning must be done in the sense of removing '\\n' and any backslashes in general.\n",
    "Also we want to manipulate this text data in order to store all the different personal statements as one text to do topic modelling with and to store them as individual statements to calculate readability and key people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "631ceb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inital removal of '\\n' and backslashes\n",
    "all_ps = all_ps.replace(\"\\n\", \" \")\n",
    "all_ps = all_ps.replace(\"\\xa0\", \"\")\n",
    "all_ps = all_ps.replace(\"\\'\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba023532",
   "metadata": {},
   "source": [
    "<b> STEP 1: Creating a dataframe fro the personal statements </b>\n",
    "- Creating a dataframe where each row accounts for a single personal statement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eaf4b4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This phrase has been used in the text file to indicate the start of a different personal statement\n",
    "individual_statements = all_ps.split('Personal statement:')\n",
    "\n",
    "ps_df = pd.DataFrame({'Personal Statements': individual_statements})\n",
    "\n",
    "#The first row is empty so it is removed\n",
    "ps_df = ps_df.drop(index=0).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dccee6",
   "metadata": {},
   "source": [
    "<b> STEP 2: Calculating Readability </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffff589d",
   "metadata": {},
   "source": [
    "Creating the Flesch-Kincaid function to calculate the readability scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c28b6620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def count_syllables(word):\n",
    "    '''\n",
    "    Helper function to count the number of sylables in a given word\n",
    "    Input: \n",
    "    word : string\n",
    "    \n",
    "    Output:\n",
    "    num_vowels : int\n",
    "    '''\n",
    "    num_vowels = len(re.findall(r'[aeiouy]+', word))\n",
    "\n",
    "    # Subtract the number of silent e's at the end of the word\n",
    "    if re.search(r'e$', word):\n",
    "        num_vowels -= 1\n",
    "\n",
    "    # Subtract one for each diphthong\n",
    "    num_vowels -= len(re.findall(r'[aeiouy]{2}', word))\n",
    "\n",
    "    # Add one if the word ends in \"-le\"\n",
    "    if re.search(r'le$', word):\n",
    "        num_vowels += 1\n",
    "\n",
    "    # Add one if the word is one syllable and ends in a consonant followed by \"y\"\n",
    "    if len(re.findall(r'^[^aeiouy]+[aeiouy]+[^aeiouy]+y$', word)):\n",
    "        num_vowels += 1\n",
    "\n",
    "    return max(1, num_vowels)\n",
    "\n",
    "def flesch_kincaid_grade(text):\n",
    "    '''\n",
    "    The function uses the Flesch-Kincaid formula to calculate the grade level required to be able to read a specificed text\n",
    "    \n",
    "    Inputs: \n",
    "    text : string\n",
    "    The text to calculate the readability level of \n",
    "    \n",
    "    Outputs: \n",
    "    grade_level : float\n",
    "    Grade age of readability \n",
    "    '''\n",
    "    text = text\n",
    "    sentences = text.split('.')\n",
    "    words = text.split()\n",
    "\n",
    "    # Calculate the average number of words per sentence\n",
    "    words_per_sentence = len(words) / len(sentences)\n",
    "\n",
    "    # Calculate the average number of syllables per word\n",
    "    syllables = 0\n",
    "    for word in words:\n",
    "        syllables += count_syllables(word)\n",
    "    syllables_per_word = syllables / len(words)\n",
    "\n",
    "    grade_level = 0.39 * words_per_sentence + 11.8 * syllables_per_word - 15.59\n",
    "    \n",
    "    grade_level = round(grade_level, 1)\n",
    "\n",
    "    return grade_level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e36965",
   "metadata": {},
   "source": [
    "Applying the function to all the personal statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fcdf95f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_df['Readability Age Grade'] = ps_df['Personal Statements'].apply(flesch_kincaid_grade)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bfe483a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Personal Statements</th>\n",
       "      <th>Readability Age Grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Taking part in a coding competition and sever...</td>\n",
       "      <td>12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mathematics and the social sciences are seen ...</td>\n",
       "      <td>14.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Science appealed to me as it offers a n...</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My interest in data analytics stems from an e...</td>\n",
       "      <td>13.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Growing up on the football pitch, I learnt t...</td>\n",
       "      <td>10.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I have always been drawn to figures and data,...</td>\n",
       "      <td>12.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data is one of the most valuable tools that t...</td>\n",
       "      <td>11.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I couldn’t believe what I saw. After looking ...</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I believe that the intersection of mathemati...</td>\n",
       "      <td>13.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Seeing how influential big data is in shapin...</td>\n",
       "      <td>13.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Personal Statements  Readability Age Grade\n",
       "0   Taking part in a coding competition and sever...                   12.5\n",
       "1   Mathematics and the social sciences are seen ...                   14.5\n",
       "2    Data Science appealed to me as it offers a n...                   14.0\n",
       "3   My interest in data analytics stems from an e...                   13.3\n",
       "4    Growing up on the football pitch, I learnt t...                   10.5\n",
       "5   I have always been drawn to figures and data,...                   12.1\n",
       "6   Data is one of the most valuable tools that t...                   11.7\n",
       "7   I couldn’t believe what I saw. After looking ...                   14.0\n",
       "8    I believe that the intersection of mathemati...                   13.6\n",
       "9    Seeing how influential big data is in shapin...                   13.8"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac151761",
   "metadata": {},
   "source": [
    "<b> STEP 3: NLP Preprocessing tasks</b> \n",
    "\n",
    "The following tasks will be carried out in this section: \n",
    "- Tokenisation\n",
    "- Stop word removal\n",
    "- Lemmatisation\n",
    "- POS tagging\n",
    "- TF-IDF\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddf6eb6",
   "metadata": {},
   "source": [
    "<b>Tokenisation:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5e83ff49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps_df['Tokens'] = ps_df['Personal Statements'].apply(lambda x: x.lower()).apply(word_tokenize)\n",
    "\n",
    "ps_df['Tokens'] = ps_df['Tokens'].apply(lambda tokens: [token for token in tokens if token not in string.punctuation])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41b278b",
   "metadata": {},
   "source": [
    "<b> Removing stop words </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "70e54841",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('English')\n",
    "\n",
    "ps_df['Filtered Tokens'] = ps_df['Tokens'].apply(lambda tokens: [token for token in tokens if token not in sw])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cd9941",
   "metadata": {},
   "source": [
    "Checking if there are any words that are still in tokens and not in the NLTK stopwords that should be taken out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "78e238b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('data', 109),\n",
       " ('’', 42),\n",
       " ('learning', 41),\n",
       " ('science', 37),\n",
       " ('using', 24),\n",
       " ('machine', 23),\n",
       " ('used', 21),\n",
       " ('social', 17),\n",
       " ('mathematics', 16),\n",
       " ('models', 15)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_token_dict = {}\n",
    "\n",
    "for row in ps_df['Filtered Tokens']:\n",
    "    for token in row:\n",
    "        if token in full_token_dict: \n",
    "            full_token_dict[token] += 1\n",
    "        else: \n",
    "            full_token_dict[token] = 1\n",
    "\n",
    "sorted_dict = sorted(full_token_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "sorted_dict[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c977cf",
   "metadata": {},
   "source": [
    "As can be seen the only tokens that were not in stop words that should be taken out are punctuation which were not caught by string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1c3ab129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking out left over punctuation\n",
    "\n",
    "ps_df['Filtered Tokens'] = ps_df['Filtered Tokens'].apply(lambda tokens: [token for token in tokens if token not in ['’', '‘', '``', \"''\", '“', '”']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a879b4",
   "metadata": {},
   "source": [
    "<b> Lemmatisation and POS Tagging </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "862cc9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "ps_df['Tagged Tokens'] = ps_df['Filtered Tokens'].apply(lambda x: pos_tag(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "74afee21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    first_letter = tag[0]\n",
    "    if first_letter == 'J': return wordnet.ADJ\n",
    "    if first_letter == 'V': return wordnet.VERB\n",
    "    if first_letter == 'N': return wordnet.NOUN\n",
    "    if first_letter == 'R': return wordnet.ADV\n",
    "    return None \n",
    "\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "def lemmatise(tokens):\n",
    "    lemmatised_tokens = []\n",
    "    for token, tag in tokens:\n",
    "        pos = get_wordnet_pos(tag)\n",
    "        if pos is None:\n",
    "            lemma = lemmatiser.lemmatize(token)\n",
    "        else:\n",
    "            lemma = lemmatiser.lemmatize(token, pos)\n",
    "        lemmatised_tokens.append(lemma)\n",
    "    return lemmatised_tokens\n",
    "\n",
    "ps_df['Lemmatised Tokens'] = ps_df['Tagged Tokens'].apply(lemmatise)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caa693b",
   "metadata": {},
   "source": [
    "<b> TF-IDF </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a2a1084d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10x1447 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2535 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "documents = []\n",
    "\n",
    "for i in range(0, 10): \n",
    "    lemmatised_tokens = ps_df.loc[i]['Lemmatised Tokens']\n",
    "    document = ' '.join(lemmatised_tokens)\n",
    "    documents.append(document)\n",
    "    \n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "tf_idf_matrix = vectorizer.fit_transform(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91de2a14",
   "metadata": {},
   "source": [
    "<b> STEP 4: Topic Modelling </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "51b1a1eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-32 {color: black;background-color: white;}#sk-container-id-32 pre{padding: 0;}#sk-container-id-32 div.sk-toggleable {background-color: white;}#sk-container-id-32 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-32 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-32 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-32 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-32 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-32 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-32 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-32 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-32 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-32 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-32 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-32 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-32 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-32 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-32 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-32 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-32 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-32 div.sk-item {position: relative;z-index: 1;}#sk-container-id-32 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-32 div.sk-item::before, #sk-container-id-32 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-32 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-32 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-32 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-32 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-32 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-32 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-32 div.sk-label-container {text-align: center;}#sk-container-id-32 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-32 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-32\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(n_components=5, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-32\" type=\"checkbox\" checked><label for=\"sk-estimator-id-32\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(n_components=5, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LatentDirichletAllocation(n_components=5, random_state=42)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "\n",
    "lda_model.fit(tf_idf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "7165b2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "data poverty social work mpi night relevant learn team science\n",
      "Topic #1:\n",
      "data use company business technology firm show math work end\n",
      "Topic #2:\n",
      "machine data model big learn theorem learning use increase practice\n",
      "Topic #3:\n",
      "data use learn machine network function problem science one understand\n",
      "Topic #4:\n",
      "science data communication lead level see social element value allocation\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "n_top_words = 10\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()[:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8649f59e",
   "metadata": {},
   "source": [
    "# Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5194bfb",
   "metadata": {},
   "source": [
    "<b> Findings from Topic modelling </b>\n",
    "\n",
    "From this it can be seen that some of the main topics discussed in successful BSc Data Science personal statements include: \n",
    "- Applications of Data Science to the social sciences\n",
    "- Work experience and data science in teh context of companies\n",
    "- Machine learning models and learning about the theory behind it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68a0f3e",
   "metadata": {},
   "source": [
    "<b> Top 10 Words</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "4217f1ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('data', 110),\n",
       " ('use', 58),\n",
       " ('learn', 45),\n",
       " ('science', 41),\n",
       " ('machine', 26),\n",
       " ('problem', 21),\n",
       " ('find', 21),\n",
       " ('model', 20),\n",
       " ('social', 17),\n",
       " ('allow', 17),\n",
       " ('think', 16)]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Top 10 words\n",
    "\n",
    "lemma_dict = {}\n",
    "\n",
    "for row in ps_df['Lemmatised Tokens']:\n",
    "    for token in row:\n",
    "        if token in lemma_dict: \n",
    "            lemma_dict[token] += 1\n",
    "        else: \n",
    "            lemma_dict[token] = 1\n",
    "\n",
    "top_words_dict = sorted(lemma_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "top_words_dict[:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ff15d7",
   "metadata": {},
   "source": [
    "Using the top 10 lemmatised words is can be seen that key topics which are talked about are data science, machine learning, applications to social science. Whilst verbs such as 'use', 'learn' 'find', 'allow' and 'think' can indicate how students applying have engaged with these topics. \n",
    "\n",
    "Although it may seem like a large abstraction these key words show that successful BSc Data Science applicants have talked a lot about Data Science and Machine learning in their personal statements as well as the social issues they realte to. Despite this seeming like an obvious insight, it is useful to keep in mind for prospective applicants who may not only be applying to Data "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
